{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract data from CSV files\n",
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    # Test if a folder path exists\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a list of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary where keys are table names and values are DataFrames containing data from each CSV file\n",
    "    # This allows us to iterate over all the tables and perform specific transformations in the transform_raw() function  \n",
    "    data_frames = {}\n",
    "\n",
    "    # Iterating over each CSV file in the folder\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"-> CSV file '{csv_file}' loaded successfully.\")\n",
    "            \n",
    "            # Add 'extracted_at' column with current timestamp\n",
    "            df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Based on the CSV Files]**\n",
    "\n",
    "**Customers**\n",
    "* SignupDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "\n",
    "**Dates**\n",
    "* DateID : will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Product Usage**\n",
    "* Date column not available.\n",
    "\n",
    "**Subscriptions**\n",
    "* StartDate,EndDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "* Status: will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Support Interactions**\n",
    "* Date column not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to transform raw data\n",
    "def transform_raw(data_frames, date_columns_map):\n",
    "    \"\"\"\n",
    "    Transform multiple raw DataFrames extracted from CSV files.\n",
    "    Perform cleaning procedures: Convert specified date columns to pandas datetime and \n",
    "    boolean columns to integer.\n",
    "\n",
    "    Args:\n",
    "        data_frames (dict): A dictionary where keys are table names and values are DataFrames \n",
    "                            containing raw data extracted from CSV files.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are \n",
    "                                 column names to convert to pandas datetime.\n",
    "        boolean_columns_map (dict): (I ommited this argument as there is no boolean column at this time) A dictionary where keys are table names and values are \n",
    "                                    column names to convert from boolean to integer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are cleaned DataFrames \n",
    "              with the specified transformations applied.\n",
    "    \"\"\"\n",
    "    # Create a dictionary where keys are table names and values are the clean DataFrames after performing specific transformations.\n",
    "    cleaned_data_frames = {}\n",
    "\n",
    "    # Iterating over all tables. If table is not in the dict returned by the extract() function, then it is skipped for transformation.\n",
    "    # df contains the actual data (in the DataFrame format) for each table\n",
    "    for table_name, df in data_frames.items():\n",
    "    \n",
    "        try:\n",
    "            if table_name in date_columns_map:\n",
    "            # we specify the date columns in each table to perform transformations.\n",
    "                # If a value of a key of date_columns_map is a single date column:\n",
    "                #   Ex: date_columns == 'SignupDate', a string.\n",
    "                # If a value of a key of date_columns_map is a list of date columns:\n",
    "                #   Ex: date_columns == ['StartDate', 'EndDate'], a list.\n",
    "                date_columns = date_columns_map[table_name] # accessing the value of the key 'table_name'\n",
    "            \n",
    "            # Note: the transform_raw() can receive a list of date columns, so we need to ensure the date_columns variable\n",
    "            # is always treated as a list, even if a single date column name is provided.\n",
    "                # If date_columns is a list, the condition is True\n",
    "                # If date_columns is not a list, the condition is False, then it transforms it into a list.\n",
    "                if not isinstance(date_columns, list):\n",
    "                    date_columns = [date_columns]\n",
    "\n",
    "                # Iterate over a potential list of columns (either single or multiple), one by one making the transformation.\n",
    "                for date_column in date_columns:\n",
    "                    # Check if the date column exists in the DataFrames that correspond to the data of each table. \n",
    "                    if date_column not in df.columns:\n",
    "                        raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                    \n",
    "                    # Format the date column to 'YYYY-MM-DD' format\n",
    "                    df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "                    print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "                    print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "                \n",
    "                # Builds a DataFrame where date columns have been cleaned for each table, which is a key of this dict.\n",
    "                # Each cleaned DataFrame is stored as a value of each table.\n",
    "            \n",
    "            cleaned_data_frames[table_name] = df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            # Indicates that an error occurred during the processing of the DataFrame for table_name and it\n",
    "            # sets to None to signify that the data transformation or cleaning for that table was unsuccessful.\n",
    "            cleaned_data_frames[table_name] = None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "            cleaned_data_frames[table_name] = None\n",
    "            \n",
    "    # Returns a clean DataFrame when dates have been treated.\n",
    "    return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to ingest CSV data into the Bronze layer using DataFrame.to_sql\n",
    "# Note: we use if_exists='replace'. This performs a full refresh of the table content (drop the table and ingest last updated data).\n",
    "# Note: use if_exists='append' is you want to append data for the specific table.\n",
    "def ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name, table_name, date_columns_map):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into the Bronze layer of a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        table_name (str): Name of the table to ingest data into.\n",
    "        date_columns (list or str): Name(s) of the column(s) to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Ingest Function.\")\n",
    "\n",
    "    # Calling the Extract Function for all CSV files\n",
    "    # Returns: raw_data_dfs, a dict where keys are table names and values are DataFrames with raw data extracted from CSV files.\n",
    "    print(\"Extract Function.\")\n",
    "    raw_data_dfs = extract(csv_folder_path)\n",
    "    if raw_data_dfs is None:\n",
    "        # Handle case where extraction fails\n",
    "        print(\"Extraction failed.\")\n",
    "        return\n",
    "\n",
    "    # Calling the Transformation Function\n",
    "    # For each table stored in raw_data_dfs, the transform_raw() perform specific date transformation for each date column.\n",
    "    # The date_columns_map is a dict where the key is the table name and the values are single date columns (in string format)\n",
    "    # or multiple columns (in list format). Recall that the transform_raw() also ensures that the date columns are always treated\n",
    "    # as a list for consistency.\n",
    "    # It returns transformed_data_dfs, where keys are table names and values are the cleaned DataFrames.\n",
    "    print(\"Transformation Function.\")\n",
    "    transformed_data_dfs = transform_raw(raw_data_dfs, date_columns_map)\n",
    "    if transformed_data_dfs is None:\n",
    "        # Handle case where transformation fails\n",
    "        print(\"Error occurred during transformation. Processing aborted.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create the database engine\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with db_engine.connect() as connection:\n",
    "            # Check if the schema exists\n",
    "            result = connection.execute(\n",
    "                text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {\"schema\": schema_name}\n",
    "            )\n",
    "            schema_exists = result.fetchone() is not None\n",
    "            if not schema_exists:\n",
    "                raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "            # Set the search path to the specified schema\n",
    "            connection.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "            print(f\"Search path set to schema '{schema_name}'.\")\n",
    "            \n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Ingesting data into {schema_name}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp column\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Ingest data into the specified schema and table\n",
    "                # Note: if_exists='replace' performs a full refresh of the table content (drop the table and ingest last updated data).\n",
    "                # Note: use if_exists='append' is you want to append data for the specific table.\n",
    "                cleaned_data_df.to_sql(table_name, db_engine, schema=schema_name, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"CSV data ingested successfully into {schema_name}.{table_name}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest Function.\n",
      "Extract Function.\n",
      "-> CSV file 'customers.csv' loaded successfully.\n",
      "-> CSV file 'dates.csv' loaded successfully.\n",
      "-> CSV file 'products.csv' loaded successfully.\n",
      "-> CSV file 'product_usage.csv' loaded successfully.\n",
      "-> CSV file 'subscriptions.csv' loaded successfully.\n",
      "-> CSV file 'support_interactions.csv' loaded successfully.\n",
      "Transformation Function.\n",
      "Successfully converted column 'SignupDate' to 'YYYY-MM-DD' format for table 'customers'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'Date' to 'YYYY-MM-DD' format for table 'dates'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'StartDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'EndDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' verified to exist.\n",
      "Search path set to schema 'bronze'.\n",
      "Ingesting data into bronze.customers...\n",
      "CSV data ingested successfully into bronze.customers.\n",
      "Ingesting data into bronze.dates...\n",
      "CSV data ingested successfully into bronze.dates.\n",
      "Ingesting data into bronze.products...\n",
      "CSV data ingested successfully into bronze.products.\n",
      "Ingesting data into bronze.product_usage...\n",
      "CSV data ingested successfully into bronze.product_usage.\n",
      "Ingesting data into bronze.subscriptions...\n",
      "CSV data ingested successfully into bronze.subscriptions.\n",
      "Ingesting data into bronze.support_interactions...\n",
      "CSV data ingested successfully into bronze.support_interactions.\n",
      "All data ingested successfully into the Bronze Layer!\n"
     ]
    }
   ],
   "source": [
    "# Bronze Ingestion parameters\n",
    "csv_folder_path = '/workspace/data/raw'\n",
    "schema_name_bronze = 'bronze'\n",
    "\n",
    "# Tables in each schema\n",
    "tables_in_bronze = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "# tables_in_silver = ['customer_transactions', 'customer_activity']\n",
    "# tables_in_gold = ['customer_segmentation', 'churn_prediction']\n",
    "\n",
    "# Define date columns for each table\n",
    "date_columns_map = {\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "# Ingest data into Bronze layer\n",
    "ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name_bronze, tables_in_bronze, date_columns_map)\n",
    "# for table_name in tables_in_bronze:\n",
    "#     print(f\"Ingesting '{table_name}' into {schema_name_bronze}...\")\n",
    "#     ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name_bronze, table_name, date_columns_map[table_name])\n",
    "\n",
    "# Ingest data into Silver layer\n",
    "# for table_name in tables_in_silver:\n",
    "#     print(f\"Ingesting '{table_name}' from {schema_name_bronze} to {schema_name_silver}...\")\n",
    "#     ingest_bronze_to_silver(connection_uri, schema_name_bronze, schema_name_silver, date_columns_map[table_name])\n",
    "\n",
    "# Ingest data into Gold layer\n",
    "# for table_name in tables_in_gold:\n",
    "#     print(f\"Ingesting '{table_name}' from {schema_name_silver} to {schema_name_gold}...\")\n",
    "#     ingest_silver_to_gold(connection_uri, schema_name_silver, schema_name_gold, date_columns_map[table_name])\n",
    "\n",
    "print(\"All data ingested successfully into the Bronze Layer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
