{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "from datetime import datetime \n",
    "import os\n",
    "from subprocess import call\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, INT, VARCHAR, DATE, TIMESTAMP, DECIMAL\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Schemas, Tables, and Views in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing SQL Scripts Against PostgreSQL (Schemas, Tables, Views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run SQL script using shell command\n",
    "# I had to pass the env parameters explicitly  to the subprocess.call() -> (PGPASSWORD, PGUSER, PGHOST, PGPORT, PGDATABASE)\n",
    "# This avoided Jupyter Notebook asking for password. \n",
    "def run_sql_script(script_name):\n",
    "    script_path = f\"/workspace/sql_scripts/{script_name}\"\n",
    "    command = f\"psql -U {user} -d {db_name} -h {host} -p {port} -f {script_path}\"\n",
    "    return call(command, shell=True, env={\n",
    "                                        'PGPASSWORD': password,\n",
    "                                        'PGUSER': user,\n",
    "                                        'PGHOST': host,\n",
    "                                        'PGPORT': port,\n",
    "                                        'PGDATABASE': db_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Schemas Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check schema existence\n",
    "def check_schema_existence(connection_uri, schema_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Schemas exist in the database ---\")\n",
    "            for schema_name in schema_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if schema_exists:\n",
    "                    print(f\"Schema '{schema_name}' exists in the database.\")\n",
    "                else:\n",
    "                    print(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(\"----- End of Schema Checking -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Tables Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check table existence\n",
    "def check_table_existence(connection_uri, schema_name, table_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Tables exist ---\")\n",
    "            for table_name in table_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = :schema AND table_name = :table\"),\n",
    "                    {\"schema\": schema_name, \"table\": table_name}\n",
    "                )\n",
    "                table_exists = result.fetchone() is not None\n",
    "                if table_exists:\n",
    "                    print(f\"Table '{table_name}' exists in schema '{schema_name}'.\")\n",
    "                else:\n",
    "                    print(f\"Table '{table_name}' does not exist in schema '{schema_name}'.\")\n",
    "            print(\"----- End of Checking Tables -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping columns between Bronze and Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bronze_columns(table_name):\n",
    "    \"\"\"\n",
    "    Maps column names from CSV dataframes to corresponding column names in bronze tables.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table for which column mapping is required.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names in CSV dataframes to their corresponding column names in bronze tables.\n",
    "    \"\"\"\n",
    "    if table_name == 'customers':\n",
    "        return {\n",
    "            'CustomerID': 'customer_id',\n",
    "            'Name': 'name',\n",
    "            'Age': 'age',\n",
    "            'Gender': 'gender',\n",
    "            'SignupDate': 'signup_date',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'subscriptions':\n",
    "        return {\n",
    "            'SubscriptionID': 'subscription_id',\n",
    "            'CustomerID': 'customer_id',\n",
    "            'StartDate': 'start_date',\n",
    "            'EndDate': 'end_date',\n",
    "            'Type': 'type',\n",
    "            'Status': 'status',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'product_usage':\n",
    "        return {\n",
    "            'UsageID': 'usage_id',\n",
    "            'CustomerID': 'customer_id',\n",
    "            'DateID': 'date_id',\n",
    "            'ProductID': 'product_id',\n",
    "            'NumLogins': 'num_logins',\n",
    "            'Amount': 'amount',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'support_interactions':\n",
    "        return {\n",
    "            'InteractionID': 'interaction_id',\n",
    "            'CustomerID': 'customer_id',\n",
    "            'DateID': 'date_id',\n",
    "            'IssueType': 'issue_type',\n",
    "            'ResolutionTime': 'resolution_time',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'dates':\n",
    "        return {\n",
    "            'DateID': 'date_id',\n",
    "            'Date': 'date',\n",
    "            'Week': 'week',\n",
    "            'Month': 'month',\n",
    "            'Quarter': 'quarter',\n",
    "            'Year': 'year',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'products':\n",
    "        return {\n",
    "            'ProductID': 'product_id',\n",
    "            'ProductName': 'product_name',\n",
    "            'Category': 'category',\n",
    "            'Price': 'price',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Table '{table_name}' not found in the bronze layer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Tables Column Names in the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_table_columns(connection_uri, schema_name, tables_in_schema):\n",
    "    \"\"\"\n",
    "    Fetches column names for a set of tables in a specified schema from a database.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The database connection URI.\n",
    "        schema_name (str): The schema name where the tables are located.\n",
    "        tables_in_silver (list of str): A list of table names for which the column names are to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are table names and the values are lists of column names for each table.\n",
    "    \"\"\"\n",
    "    columns_dict = {}\n",
    "    try:\n",
    "        engine = create_db_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            for table_name in tables_in_schema:\n",
    "                query = text(f\"\"\"\n",
    "                    SELECT column_name \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema = '{schema_name}' \n",
    "                    AND table_name = '{table_name}';\n",
    "                \"\"\")\n",
    "                result = connection.execute(query)\n",
    "                columns = [row[0] for row in result]  # Extract the first element (column_name) and create a list columns of column_names\n",
    "                columns_dict[table_name] = columns # Fill the columns_dict with keys (table_name) and values (list of column names) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching view columns: {str(e)}\")\n",
    "\n",
    "    return columns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Bronze Tables Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bronze_table_data_types():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with data types for columns in bronze tables.\n",
    "    \"\"\"\n",
    "    bronze_data_types = {\n",
    "        'customers': {\n",
    "            'CustomerID': INT,\n",
    "            'Name': VARCHAR(100),\n",
    "            'Age': INT,\n",
    "            'Gender': VARCHAR(10),\n",
    "            'SignupDate': DATE,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'dates': {\n",
    "            'DateID': INT,\n",
    "            'Date': DATE,\n",
    "            'Week': INT,\n",
    "            'Month': INT,\n",
    "            'Quarter': INT,\n",
    "            'Year': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'product_usage': {\n",
    "            'UsageID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'DateID': INT,\n",
    "            'ProductID': INT,\n",
    "            'NumLogins': INT,\n",
    "            'Amount': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'products': {\n",
    "            'ProductID': INT,\n",
    "            'ProductName': VARCHAR(100),\n",
    "            'Category': VARCHAR(50),\n",
    "            'Price': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'subscriptions': {\n",
    "            'SubscriptionID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'StartDate': DATE,\n",
    "            'EndDate': DATE,\n",
    "            'Type': VARCHAR(50),\n",
    "            'Status': VARCHAR(50),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'support_interactions': {\n",
    "            'InteractionID': INT,\n",
    "            'CustomerID': INT,\n",
    "            'DateID': INT,\n",
    "            'IssueType': VARCHAR(100),\n",
    "            'ResolutionTime': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        }\n",
    "    }\n",
    "    return bronze_data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Silver Table Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silver_table_data_types():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with data types for columns in silver tables.\n",
    "    \"\"\"\n",
    "    silver_data_types = {\n",
    "        'dim_customers': {\n",
    "            'customer_id': INT,\n",
    "            'name': VARCHAR(100),\n",
    "            'age': INT,\n",
    "            'gender': VARCHAR(10),\n",
    "            'signup_date': DATE,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'dim_dates': {\n",
    "            'date_id': INT,\n",
    "            'date': DATE,\n",
    "            'week': INT,\n",
    "            'month': INT,\n",
    "            'quarter': INT,\n",
    "            'year': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'fact_product_usage': {\n",
    "            'usage_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'date_id': INT,\n",
    "            'product_id': INT,\n",
    "            'num_logins': INT,\n",
    "            'amount': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'dim_products': {\n",
    "            'product_id': INT,\n",
    "            'product_name': VARCHAR(100),\n",
    "            'category': VARCHAR(50),\n",
    "            'price': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'fact_subscriptions': {\n",
    "            'subscription_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'start_date': DATE,\n",
    "            'end_date': DATE,\n",
    "            'type': VARCHAR(50),\n",
    "            'status': VARCHAR(50),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'fact_support_interactions': {\n",
    "            'interaction_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'date_id': INT,\n",
    "            'issue_type': VARCHAR(100),\n",
    "            'resolution_time': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        }\n",
    "    }\n",
    "    return silver_data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Gold Table Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract data from CSV files\n",
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    # Test if a folder path exists\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a list of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary where keys are table names and values are DataFrames containing data from each CSV file\n",
    "    # This allows us to iterate over all the tables and perform specific transformations in the transform_raw() function  \n",
    "    data_frames = {}\n",
    "\n",
    "    # Iterating over each CSV file in the folder\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"-> CSV file '{csv_file}' loaded successfully.\")\n",
    "            \n",
    "            # Add 'extracted_at' column with current timestamp\n",
    "            df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Based on the CSV Files]**\n",
    "\n",
    "1) Date Columns were converted to conform to PostgreSQL syntax.\n",
    "2) Column Names were renamed (The tables in the Silver Layer will contain renamed columns. See below).\n",
    "\n",
    "*Customers*\n",
    "* SignupDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "\n",
    "*Dates*\n",
    "* DateID : will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "*Product Usage*\n",
    "* Date column not available.\n",
    "\n",
    "*Subscriptions*\n",
    "* StartDate,EndDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "* Status: will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "*Support Interactions*\n",
    "* Date column not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_raw(dfs, date_columns_map):\n",
    "    \"\"\"\n",
    "    Transform multiple raw DataFrames extracted from CSV files.\n",
    "    Perform cleaning procedures: Convert specified date columns to pandas datetime and \n",
    "    rename columns using map_bronze_columns function.\n",
    "\n",
    "    Args:\n",
    "        data_frames (dict): A dictionary where keys are table names and values are DataFrames \n",
    "                            containing raw data extracted from CSV files.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are \n",
    "                                 column names to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are cleaned DataFrames \n",
    "              with the specified transformations applied.\n",
    "    \"\"\"\n",
    "    # Create a dictionary where keys are table names and values are the clean DataFrames after performing specific transformations.\n",
    "    cleaned_data_frames = {}\n",
    "\n",
    "    # Iterate over all tables. If table is not in the dict returned by the extract() function, then it is skipped for transformation.\n",
    "    # df contains the actual data (in the DataFrame format) for each table\n",
    "    for table_name, df in dfs.items():\n",
    "    \n",
    "        try:\n",
    "            if table_name in date_columns_map:\n",
    "                # We specify the date columns in each table to perform transformations.\n",
    "                # If a value of a key of date_columns_map is a single date column:\n",
    "                #   Ex: date_columns == 'SignupDate', a string.\n",
    "                # If a value of a key of date_columns_map is a list of date columns:\n",
    "                #   Ex: date_columns == ['StartDate', 'EndDate'], a list.\n",
    "                date_columns = date_columns_map[table_name]  # accessing the value of the key 'table_name'\n",
    "            \n",
    "                # Note: the transform_raw() can receive a list of date columns, so we need to ensure the date_columns variable\n",
    "                # is always treated as a list, even if a single date column name is provided.\n",
    "                if not isinstance(date_columns, list):\n",
    "                    date_columns = [date_columns]\n",
    "\n",
    "                # Iterate over a potential list of columns (either single or multiple), one by one making the transformation.\n",
    "                for date_column in date_columns:\n",
    "                    # Check if the date column exists in the DataFrames that correspond to the data of each table. \n",
    "                    if date_column not in df.columns:\n",
    "                        raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                    \n",
    "                    # Format the date column to 'YYYY-MM-DD' format\n",
    "                    df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "                    print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "                    print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "\n",
    "            # Rename columns using the map_bronze_columns function\n",
    "            column_mapping = map_bronze_columns(table_name)\n",
    "            df.rename(columns=column_mapping, inplace=True)\n",
    "            print(f\"Columns renamed for table '{table_name}'.\")\n",
    "\n",
    "            # Add the cleaned DataFrame to the cleaned_data_frames dictionary\n",
    "            cleaned_data_frames[table_name] = df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            # Indicates that an error occurred during the processing of the DataFrame for table_name and it\n",
    "            # sets to None to signify that the data transformation or cleaning for that table was unsuccessful.\n",
    "            cleaned_data_frames[table_name] = None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "            cleaned_data_frames[table_name] = None\n",
    "            \n",
    "    # Returns a clean DataFrame when dates have been treated adn columns have been renamed.\n",
    "    return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion into the Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_bronze(dfs, connection_uri, bronze_schema_name):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into the Bronze layer of a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are columns to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are DataFrames with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----- Ingesting Data Into Bronze Layer. -----\")\n",
    "\n",
    "    try:\n",
    "        # Create the database engine\n",
    "        engine = create_engine(connection_uri)\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Set the search path to the specified schema\n",
    "            set_search_path_query = text(f\"SET search_path TO {bronze_schema_name};\")\n",
    "            connection.execute(set_search_path_query)\n",
    "            print(f\"Search path set to schema '{bronze_schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            print(\"-- to_sql() Ingestion Procedure in Bronze. --\")\n",
    "            for table_name, cleaned_data_df in dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Ingesting data into {bronze_schema_name}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp columns\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Get data types for the table from the dictionary\n",
    "                bronze_data_types = get_bronze_table_data_types()\n",
    "                data_type_dict = bronze_data_types.get(table_name)\n",
    "\n",
    "                if data_type_dict is None:\n",
    "                    raise ValueError(f\"Data types not found for table '{table_name}' in bronze layer.\")\n",
    "\n",
    "                # Ingest data into the specified schema and table with specified data types\n",
    "                cleaned_data_df.to_sql(table_name, engine, schema=bronze_schema_name, if_exists='replace', index=False, dtype=data_type_dict)\n",
    "\n",
    "                print(f\"-> CSV data ingested successfully into {bronze_schema_name}.{table_name}.\")\n",
    "\n",
    "        print(\"----- END OF DATA INGESTION INTO BRONZE -----\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration and Transformation into the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_bronze_to_silver(transformed_data_dfs, connection_uri, silver_schema_name, tables_in_silver, create_silver_tables_script_path):\n",
    "    print(\"----- Ingesting Data Into Silver Layer. -----\")\n",
    "\n",
    "    try:\n",
    "        # Create database engine\n",
    "        engine = create_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        print(\"Database engine created successfully.\")\n",
    "\n",
    "        # 6) (Silver) Run create_silver_tables.sql\n",
    "        print(\"----- Creating TABLES in Silver -----\")\n",
    "        result = run_sql_script(create_silver_tables_script_path)\n",
    "        if result == 0:\n",
    "            print(\"SQL script executed successfully. Tables were created in the Siver Layer.\")\n",
    "            # Check if schemas exist in the database\n",
    "            check_table_existence(connection_uri, silver_schema_name, tables_in_silver)\n",
    "        else:\n",
    "            print(\"Error executing SQL script.\")\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Check if the silver schema exists\n",
    "            schema_exists_query = text(f\"\"\"\n",
    "                SELECT schema_name \n",
    "                FROM information_schema.schemata \n",
    "                WHERE schema_name = :schema_name\n",
    "            \"\"\")\n",
    "            result = connection.execute(schema_exists_query, {\"schema_name\": silver_schema_name})\n",
    "            if result.fetchone() is None:\n",
    "                raise ValueError(f\"Schema '{silver_schema_name}' does not exist in the database.\")\n",
    "            \n",
    "            # Set the search path to the silver schema\n",
    "            connection.execute(text(f\"SET search_path TO {silver_schema_name};\"))\n",
    "            print(f\"Search path set to schema '{silver_schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database tables\n",
    "            for silver_table_name, (temp, cleaned_data_df) in zip(tables_in_silver, transformed_data_dfs.items()):\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{silver_table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "\n",
    "                # Add 'inserted_at' timestamp columns\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Get data types for the table from the dictionary\n",
    "                silver_data_types = get_silver_table_data_types()\n",
    "                data_type_dict = silver_data_types.get(silver_table_name)\n",
    "\n",
    "                if data_type_dict is None:\n",
    "                    raise ValueError(f\"Data types not found for table '{silver_table_name}' in silver layer.\")\n",
    "\n",
    "                # Ingest data into the specified schema and table with specified data types\n",
    "                cleaned_data_df.to_sql(silver_table_name, engine, schema=silver_schema_name, if_exists='replace', index=False, dtype=data_type_dict)\n",
    "\n",
    "                print(f\"-> Data data ingested successfully into {silver_schema_name}.{silver_table_name}.\")\n",
    "    \n",
    "        print(\"----- END OF DATA INGESTION IN SILVER -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"SQLAlchemyError occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "# def ingest_bronze_to_silver(connection_uri, bronze_schema_name, silver_schema_name, transformed_data_dfs, tables_in_silver):\n",
    "#     print(\"----- Ingesting Data Into Silver Layer. -----\")\n",
    "\n",
    "#     try:\n",
    "#         # Create database engine\n",
    "#         engine = create_engine(connection_uri)\n",
    "#         if engine is None:\n",
    "#             print(\"Failed to create the database engine.\")\n",
    "#             return\n",
    "\n",
    "#         print(\"Database engine created successfully.\")\n",
    "\n",
    "#         # Verify connection and schema existence\n",
    "#         with engine.connect() as connection:\n",
    "#             # Check if the silver schema exists\n",
    "#             schema_exists_query = text(f\"\"\"\n",
    "#                 SELECT schema_name \n",
    "#                 FROM information_schema.schemata \n",
    "#                 WHERE schema_name = :schema_name\n",
    "#             \"\"\")\n",
    "#             result = connection.execute(schema_exists_query, {\"schema_name\": silver_schema_name})\n",
    "#             if result.fetchone() is None:\n",
    "#                 raise ValueError(f\"Schema '{silver_schema_name}' does not exist in the database.\")\n",
    "            \n",
    "#             # Set the search path to the silver schema\n",
    "#             connection.execute(text(f\"SET search_path TO {silver_schema_name};\"))\n",
    "#             print(f\"Search path set to schema '{silver_schema_name}'.\")\n",
    "\n",
    "#             # Iterate over transformed DataFrames and ingest data into the database tables\n",
    "#             # print(\"-- Ingestion Procedure in Silver. --\")\n",
    "#             # for silver_table_name, (bronze_table_name, cleaned_data_df) in zip(tables_in_silver, transformed_data_dfs.items()):\n",
    "#             #     if cleaned_data_df is None:\n",
    "#             #         print(f\"Skipping ingestion for table '{silver_table_name}' due to previous errors.\")\n",
    "#             #         continue\n",
    "#             print(\"-- to_sql() Ingestion Procedure in Silver. --\")\n",
    "#             for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "#                 if cleaned_data_df is None:\n",
    "#                     print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "#                     continue\n",
    "\n",
    "#                 # Add 'extracted_at' and 'inserted_at' timestamp columns for the ingestion into Silver\n",
    "#                 cleaned_data_df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#                 cleaned_data_df['inserted_at']  = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#                 # Ingest data into the specified schema and table using to_sql\n",
    "#                 cleaned_data_df.to_sql(table_name, engine, schema=silver_schema_name, if_exists='replace', index=False, dtype=get_silver_table_data_types().get(silver_table_name))\n",
    "\n",
    "#                 print(f\"-> Data ingested successfully into {silver_schema_name}.{silver_table_name}.\")\n",
    "    \n",
    "#         print(\"----- END OF DATA TRANSFORMATION IN SILVER -----\")\n",
    "#         # return aggregated_data_dfs\n",
    "    \n",
    "#     except SQLAlchemyError as e:\n",
    "#         print(f\"SQLAlchemyError occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"ValueError: {str(ve)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation and Feature Engineering into the Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ingest_silver_to_gold(connection_uri, silver_schema_name, gold_schema_name, tables_in_gold, create_gold_tables_script_path):\n",
    "#     print(\"----- Ingesting Data Into Gold Layer. -----\")\n",
    "\n",
    "#     try:\n",
    "#         # Create database engine\n",
    "#         engine = create_engine(connection_uri)\n",
    "#         if engine is None:\n",
    "#             print(\"Failed to create the database engine.\")\n",
    "#             return\n",
    "\n",
    "#         print(\"Database engine created successfully.\")\n",
    "        \n",
    "#         # Verify connection and schema existence\n",
    "#         with engine.connect() as connection:\n",
    "#             # Check if the gold schema exists\n",
    "#             schema_exists_query = text(f\"\"\"\n",
    "#                 SELECT schema_name \n",
    "#                 FROM information_schema.schemata \n",
    "#                 WHERE schema_name = :schema_name\n",
    "#             \"\"\")\n",
    "#             result = connection.execute(schema_exists_query, {\"schema_name\": gold_schema_name})\n",
    "#             if result.fetchone() is None:\n",
    "#                 raise ValueError(f\"Schema '{gold_schema_name}' does not exist in the database.\")\n",
    "            \n",
    "#             # Set the search path to the gold schema\n",
    "#             connection.execute(text(f\"SET search_path TO {gold_schema_name};\"))\n",
    "#             print(f\"Search path set to schema '{gold_schema_name}'.\")\n",
    "\n",
    "#             # Extract data using the SELECT queries embedded in the Gold SQL script\n",
    "#             print(\"-- Extracting Data for Ingestion into Gold. --\")\n",
    "#             aggregated_data_dfs = {}\n",
    "#             for table in tables_in_gold:\n",
    "#                 query = text(f\"SELECT * FROM {gold_schema_name}.{table}\")\n",
    "#                 df = pd.read_sql(query, connection)\n",
    "#                 aggregated_data_dfs[table] = df\n",
    "#                 print(f\"Data extracted for table '{table}'\")\n",
    "\n",
    "#             # Iterate over aggregated DataFrames and ingest data into the database tables\n",
    "#             print(\"-- Ingestion Procedure in Gold. --\")\n",
    "#             for gold_table_name, aggregated_data_df in aggregated_data_dfs.items():\n",
    "#                 if aggregated_data_df is None:\n",
    "#                     print(f\"Skipping ingestion for table '{gold_table_name}' due to previous errors.\")\n",
    "#                     continue\n",
    "\n",
    "#                 # Add 'extracted_at' and 'inserted_at' timestamp columns for the ingestion into Gold\n",
    "#                 aggregated_data_df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#                 aggregated_data_df['inserted_at']  = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#                 # Ingest data into the specified schema and table using to_sql\n",
    "#                 aggregated_data_df.to_sql(gold_table_name, engine, schema=gold_schema_name, if_exists='replace', index=False)\n",
    "\n",
    "#                 print(f\"-> Data ingested successfully into {gold_schema_name}.{gold_table_name}.\")\n",
    "\n",
    "#         print(\"----- END OF DATA INGESTION IN GOLD -----\")\n",
    "    \n",
    "#     except SQLAlchemyError as e:\n",
    "#         print(f\"SQLAlchemyError occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"ValueError: {str(ve)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Creating SCHEMAS in PostgreSQL -----\n",
      "CREATE SCHEMA\n",
      "CREATE SCHEMA\n",
      "CREATE SCHEMA\n",
      "SQL script executed successfully. Schemas were created.\n",
      "Database engine created successfully.\n",
      "--- Checking if Schemas exist in the database ---\n",
      "Schema 'bronze' exists in the database.\n",
      "Schema 'silver' exists in the database.\n",
      "Schema 'gold' exists in the database.\n",
      "----- End of Schema Checking -----\n",
      " -- Extract Function. --\n",
      "-> CSV file 'customers.csv' loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CSV file 'dates.csv' loaded successfully.\n",
      "-> CSV file 'products.csv' loaded successfully.\n",
      "-> CSV file 'product_usage.csv' loaded successfully.\n",
      "-> CSV file 'subscriptions.csv' loaded successfully.\n",
      "-> CSV file 'support_interactions.csv' loaded successfully.\n",
      "----- Ingesting Data Into Bronze Layer. -----\n",
      "Search path set to schema 'bronze'.\n",
      "-- to_sql() Ingestion Procedure in Bronze. --\n",
      "Ingesting data into bronze.customers...\n",
      "-> CSV data ingested successfully into bronze.customers.\n",
      "Ingesting data into bronze.dates...\n",
      "-> CSV data ingested successfully into bronze.dates.\n",
      "Ingesting data into bronze.products...\n",
      "-> CSV data ingested successfully into bronze.products.\n",
      "Ingesting data into bronze.product_usage...\n",
      "-> CSV data ingested successfully into bronze.product_usage.\n",
      "Ingesting data into bronze.subscriptions...\n",
      "-> CSV data ingested successfully into bronze.subscriptions.\n",
      "Ingesting data into bronze.support_interactions...\n",
      "-> CSV data ingested successfully into bronze.support_interactions.\n",
      "----- END OF DATA INGESTION INTO BRONZE -----\n",
      "--- PRINTING COLUMN NAMES AFTER INGESTING IN BRONZE---\n",
      "Database engine created successfully.\n",
      "dict_items([('customers', ['CustomerID', 'Name', 'Age', 'Gender', 'SignupDate', 'extracted_at', 'inserted_at']), ('dates', ['DateID', 'Date', 'Week', 'Month', 'Quarter', 'Year', 'extracted_at', 'inserted_at']), ('product_usage', ['UsageID', 'CustomerID', 'DateID', 'ProductID', 'NumLogins', 'Amount', 'extracted_at', 'inserted_at']), ('products', ['ProductID', 'ProductName', 'Category', 'Price', 'extracted_at', 'inserted_at']), ('subscriptions', ['SubscriptionID', 'CustomerID', 'StartDate', 'EndDate', 'Type', 'Status', 'extracted_at', 'inserted_at']), ('support_interactions', ['InteractionID', 'CustomerID', 'DateID', 'IssueType', 'ResolutionTime', 'extracted_at', 'inserted_at'])])\n",
      "-- Transformation Function. --\n",
      "Successfully converted column 'SignupDate' to 'YYYY-MM-DD' format for table 'customers'.\n",
      "Data type after conversion: object\n",
      "Columns renamed for table 'customers'.\n",
      "Successfully converted column 'Date' to 'YYYY-MM-DD' format for table 'dates'.\n",
      "Data type after conversion: object\n",
      "Columns renamed for table 'dates'.\n",
      "Columns renamed for table 'products'.\n",
      "Columns renamed for table 'product_usage'.\n",
      "Successfully converted column 'StartDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'EndDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Columns renamed for table 'subscriptions'.\n",
      "Columns renamed for table 'support_interactions'.\n",
      "----- Ingesting Data Into Silver Layer. -----\n",
      "Database engine created successfully.\n",
      "----- Creating TABLES in Silver -----\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "SQL script executed successfully. Tables were created in the Siver Layer.\n",
      "Database engine created successfully.\n",
      "--- Checking if Tables exist ---\n",
      "Table 'dim_customers' exists in schema 'silver'.\n",
      "Table 'dim_dates' exists in schema 'silver'.\n",
      "Table 'dim_products' exists in schema 'silver'.\n",
      "Table 'fact_product_usage' exists in schema 'silver'.\n",
      "Table 'fact_subscriptions' exists in schema 'silver'.\n",
      "Table 'fact_support_interactions' exists in schema 'silver'.\n",
      "----- End of Checking Tables -----\n",
      "Search path set to schema 'silver'.\n",
      "-> Data data ingested successfully into silver.dim_customers.\n",
      "-> Data data ingested successfully into silver.dim_dates.\n",
      "-> Data data ingested successfully into silver.dim_products.\n",
      "-> Data data ingested successfully into silver.fact_product_usage.\n",
      "-> Data data ingested successfully into silver.fact_subscriptions.\n",
      "-> Data data ingested successfully into silver.fact_support_interactions.\n",
      "----- END OF DATA INGESTION IN SILVER -----\n",
      "--- PRINTING COLUMN NAMES AFTER INGESTION IN SILVER ---\n",
      "Database engine created successfully.\n",
      "dict_items([('dim_customers', ['customer_id', 'name', 'age', 'gender', 'signup_date', 'extracted_at', 'inserted_at']), ('dim_dates', ['date_id', 'date', 'week', 'month', 'quarter', 'year', 'extracted_at', 'inserted_at']), ('dim_products', ['product_id', 'product_name', 'category', 'price', 'extracted_at', 'inserted_at']), ('fact_product_usage', ['usage_id', 'customer_id', 'date_id', 'product_id', 'num_logins', 'amount', 'extracted_at', 'inserted_at']), ('fact_subscriptions', ['subscription_id', 'customer_id', 'start_date', 'end_date', 'type', 'status', 'extracted_at', 'inserted_at']), ('fact_support_interactions', ['interaction_id', 'customer_id', 'date_id', 'issue_type', 'resolution_time', 'extracted_at', 'inserted_at'])])\n",
      "----- Creating TABLES in Gold -----\n",
      "SELECT 50\n",
      "SELECT 50\n",
      "SELECT 50\n",
      "SELECT 50\n",
      "SQL script executed successfully. Tables were created in the Gold Layer.\n",
      "Database engine created successfully.\n",
      "--- Checking if Tables exist ---\n",
      "Table 'churn_customers' exists in schema 'gold'.\n",
      "Table 'product_usage_features' exists in schema 'gold'.\n",
      "Table 'support_interaction_features' exists in schema 'gold'.\n",
      "Table 'churn_features' exists in schema 'gold'.\n",
      "----- End of Checking Tables -----\n"
     ]
    }
   ],
   "source": [
    "# Ingestion Parameters for Bronze, Silver, and Gold Layers\n",
    "csv_folder_path = '/workspace/data/raw'\n",
    "schema_names = ['bronze', 'silver', 'gold']\n",
    "bronze_schema = 'bronze'\n",
    "silver_schema = 'silver'\n",
    "gold_schema   = 'gold'\n",
    "\n",
    "create_schemas_script_path       = 'schemas/create_schemas.sql'\n",
    "create_bronze_tables_script_path = 'bronze/create_bronze_tables.sql'\n",
    "create_silver_tables_script_path = 'silver/create_silver_tables.sql'\n",
    "create_gold_tables_script_path   = 'gold/create_gold_tables.sql'\n",
    "\n",
    "# Note, do not alter the order of the table names in silver, or it will not be ingested correctly\n",
    "tables_in_bronze = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "tables_in_silver = ['dim_customers', 'dim_dates', 'dim_products', 'fact_product_usage', 'fact_subscriptions', 'fact_support_interactions']\n",
    "tables_in_gold   = ['churn_customers', 'product_usage_features', 'support_interaction_features', 'churn_features']\n",
    "\n",
    "# Define date columns for each table in Bronze to perform specific transformations to conform to PostgreSQL syntax.\n",
    "date_columns_map = {            \n",
    "    # Date columns should be follow the CSV files column names\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "# Execute functions (please, respect the order)\n",
    "    # 1) Run create_schemas.sql\n",
    "    # 2) (Bronze) Calling the Extract Function for all CSV files\n",
    "    # 3) (Bronze) Data Ingestion with Minor Transformation\n",
    "    # 4) (Silver) Calling the Transformation Function for the DataFramese based on the CSV files\n",
    "    # 5) (Silver) Data Integration and Transformation\n",
    "    # 6) (Gold) Data Aggregation and Feature Engineering - Run create_gold_tables.sql\n",
    "\n",
    "# 1) Run create_schemas.sql \n",
    "print(\"----- Creating SCHEMAS in PostgreSQL -----\")\n",
    "result = run_sql_script(create_schemas_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Schemas were created.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# 2) (Bronze) Calling the Extract Function for all CSV files\n",
    "print(\" -- Extract Function. --\")\n",
    "raw_data_dfs = extract(csv_folder_path)\n",
    "if raw_data_dfs is None:\n",
    "    print(\"Extraction failed.\")\n",
    "\n",
    "# 3) (Bronze) Data Ingestion with Minor Transformation\n",
    "# transformed_data_dfs = ingest_csv_to_bronze(csv_folder_path, connection_uri, bronze_schema, date_columns_map)\n",
    "ingest_csv_to_bronze(raw_data_dfs, connection_uri, bronze_schema)\n",
    "\n",
    "# TEST COLUMN NAMES BEFORE INGESTION IN BRONZE\n",
    "print(\"--- PRINTING COLUMN NAMES AFTER INGESTING IN BRONZE---\")\n",
    "bronze_column_names_before_ingestion = get_schema_table_columns(connection_uri, bronze_schema, tables_in_bronze)\n",
    "print(bronze_column_names_before_ingestion.items())\n",
    "\n",
    "# 4) Calling the Transformation Function for the DataFramese based on the CSV files\n",
    "print(\"-- Transformation Function. --\")\n",
    "transformed_data_dfs = transform_raw(raw_data_dfs, date_columns_map)\n",
    "if transformed_data_dfs is None:\n",
    "    print(\"Error occurred during transformation. Processing aborted.\")\n",
    "\n",
    "# 5) (Silver) Data Integration and Transformation\n",
    "ingest_bronze_to_silver(transformed_data_dfs, connection_uri, silver_schema, tables_in_silver, create_silver_tables_script_path)\n",
    "\n",
    "# TEST COLUMN NAMES AFTER INGESTION IN SILVER\n",
    "print(\"--- PRINTING COLUMN NAMES AFTER INGESTION IN SILVER ---\")\n",
    "silver_columns_after_ingestion = get_schema_table_columns(connection_uri, silver_schema, tables_in_silver)\n",
    "print(silver_columns_after_ingestion.items())\n",
    "\n",
    "# 6) (Gold) Data Aggregation and Feature Engineering - Run create_gold_tables.sql\n",
    "print(\"----- Creating TABLES in Gold -----\")\n",
    "result = run_sql_script(create_gold_tables_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Tables were created in the Gold Layer.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_table_existence(connection_uri, gold_schema, tables_in_gold)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# # 7) (Gold) Data Aggregation and Feature Engineering\n",
    "# ingest_silver_to_gold(connection_uri, silver_schema, gold_schema, tables_in_gold, create_gold_tables_script_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
