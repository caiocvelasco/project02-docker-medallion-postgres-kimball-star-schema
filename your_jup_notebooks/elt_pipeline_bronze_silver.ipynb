{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "from datetime import datetime \n",
    "import os\n",
    "from subprocess import call\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Schemas, Tables, and Views in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Schemas in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run SQL script using shell command\n",
    "# I had to pass the env parameters explicitly  to the subprocess.call() -> (PGPASSWORD, PGUSER, PGHOST, PGPORT, PGDATABASE)\n",
    "# This avoided Jupyter Notebook asking for password. \n",
    "def run_sql_script(script_name):\n",
    "    script_path = f\"/workspace/sql_scripts/{script_name}\"\n",
    "    print(f\"{user}:{password}@{host}/{db_name}\")\n",
    "    command = f\"psql -U {user} -d {db_name} -h {host} -p {port} -f {script_path}\"\n",
    "    return call(command, shell=True, env={\n",
    "                                        'PGPASSWORD': password,\n",
    "                                        'PGUSER': user,\n",
    "                                        'PGHOST': host,\n",
    "                                        'PGPORT': port,\n",
    "                                        'PGDATABASE': db_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Schemas Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check schema existence\n",
    "def check_schema_existence(connection_uri, schema_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            for schema_name in schema_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if schema_exists:\n",
    "                    print(f\"Schema '{schema_name}' exists in the database.\")\n",
    "                else:\n",
    "                    print(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tables in the Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to execute /workspace/sql_scripts/bronze/create_bronze_tables.sql\n",
    "# def create_bronze_tables(connection_uri, bronze_schema, script_path):\n",
    "#     try:\n",
    "#         with open(script_path, 'r') as file:\n",
    "#             print(f\"File '{script_path}' opened successfully.\")\n",
    "#             sql_script = file.read()\n",
    "        \n",
    "#         engine = create_db_engine(connection_uri)\n",
    "#         with engine.connect() as connection:\n",
    "#             # Set the search path to the bronze schema\n",
    "#             connection.execute(text(f\"SET search_path TO {bronze_schema};\"))\n",
    "#             # Execute the script to create tables\n",
    "#             connection.execute(text(sql_script))      \n",
    "#         print(\"Bronze tables created successfully.\")\n",
    "        \n",
    "#     except FileNotFoundError:\n",
    "#         print(\"create_bronze_tables.sql not found.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing create_bronze_tables.sql: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Views in the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to execute create_silver_views.sql\n",
    "# def create_silver_views(connection_uri, silver_schema, script_path):\n",
    "#     try:\n",
    "#         with open(script_path, 'r') as file:\n",
    "#             sql_script = file.read()\n",
    "        \n",
    "#         engine = create_db_engine(connection_uri)\n",
    "#         with engine.connect() as connection:\n",
    "#             # Set the search path to the silver schema\n",
    "#             connection.execute(text(f\"SET search_path TO {silver_schema};\"))\n",
    "#             # Execute the script to create views\n",
    "#             connection.execute(text(sql_script))\n",
    "            \n",
    "#         print(\"Silver views created successfully.\")\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"create_silver_views.sql not found.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing create_silver_views.sql: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion into the Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract data from CSV files\n",
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    # Test if a folder path exists\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a list of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary where keys are table names and values are DataFrames containing data from each CSV file\n",
    "    # This allows us to iterate over all the tables and perform specific transformations in the transform_raw() function  \n",
    "    data_frames = {}\n",
    "\n",
    "    # Iterating over each CSV file in the folder\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"-> CSV file '{csv_file}' loaded successfully.\")\n",
    "            \n",
    "            # Add 'extracted_at' column with current timestamp\n",
    "            df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Based on the CSV Files]**\n",
    "\n",
    "**Customers**\n",
    "* SignupDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "\n",
    "**Dates**\n",
    "* DateID : will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Product Usage**\n",
    "* Date column not available.\n",
    "\n",
    "**Subscriptions**\n",
    "* StartDate,EndDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "* Status: will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Support Interactions**\n",
    "* Date column not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to transform raw data\n",
    "def transform_raw(data_frames, date_columns_map):\n",
    "    \"\"\"\n",
    "    Transform multiple raw DataFrames extracted from CSV files.\n",
    "    Perform cleaning procedures: Convert specified date columns to pandas datetime and \n",
    "    boolean columns to integer.\n",
    "\n",
    "    Args:\n",
    "        data_frames (dict): A dictionary where keys are table names and values are DataFrames \n",
    "                            containing raw data extracted from CSV files.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are \n",
    "                                 column names to convert to pandas datetime.\n",
    "        boolean_columns_map (dict): (I ommited this argument as there is no boolean column at this time) A dictionary where keys are table names and values are \n",
    "                                    column names to convert from boolean to integer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are cleaned DataFrames \n",
    "              with the specified transformations applied.\n",
    "    \"\"\"\n",
    "    # Create a dictionary where keys are table names and values are the clean DataFrames after performing specific transformations.\n",
    "    cleaned_data_frames = {}\n",
    "\n",
    "    # Iterating over all tables. If table is not in the dict returned by the extract() function, then it is skipped for transformation.\n",
    "    # df contains the actual data (in the DataFrame format) for each table\n",
    "    for table_name, df in data_frames.items():\n",
    "    \n",
    "        try:\n",
    "            if table_name in date_columns_map:\n",
    "            # we specify the date columns in each table to perform transformations.\n",
    "                # If a value of a key of date_columns_map is a single date column:\n",
    "                #   Ex: date_columns == 'SignupDate', a string.\n",
    "                # If a value of a key of date_columns_map is a list of date columns:\n",
    "                #   Ex: date_columns == ['StartDate', 'EndDate'], a list.\n",
    "                date_columns = date_columns_map[table_name] # accessing the value of the key 'table_name'\n",
    "            \n",
    "            # Note: the transform_raw() can receive a list of date columns, so we need to ensure the date_columns variable\n",
    "            # is always treated as a list, even if a single date column name is provided.\n",
    "                # If date_columns is a list, the condition is True\n",
    "                # If date_columns is not a list, the condition is False, then it transforms it into a list.\n",
    "                if not isinstance(date_columns, list):\n",
    "                    date_columns = [date_columns]\n",
    "\n",
    "                # Iterate over a potential list of columns (either single or multiple), one by one making the transformation.\n",
    "                for date_column in date_columns:\n",
    "                    # Check if the date column exists in the DataFrames that correspond to the data of each table. \n",
    "                    if date_column not in df.columns:\n",
    "                        raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                    \n",
    "                    # Format the date column to 'YYYY-MM-DD' format\n",
    "                    df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "                    print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "                    print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "                \n",
    "                # Builds a DataFrame where date columns have been cleaned for each table, which is a key of this dict.\n",
    "                # Each cleaned DataFrame is stored as a value of each table.\n",
    "            \n",
    "            cleaned_data_frames[table_name] = df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            # Indicates that an error occurred during the processing of the DataFrame for table_name and it\n",
    "            # sets to None to signify that the data transformation or cleaning for that table was unsuccessful.\n",
    "            cleaned_data_frames[table_name] = None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "            cleaned_data_frames[table_name] = None\n",
    "            \n",
    "    # Returns a clean DataFrame when dates have been treated.\n",
    "    return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to ingest CSV data into the Bronze layer using DataFrame.to_sql\n",
    "# Note: we use if_exists='replace'. This performs a full refresh of the table content (drop the table and ingest last updated data).\n",
    "# Note: use if_exists='append' is you want to append data for the specific table.\n",
    "def ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name, table_name, date_columns_map):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into the Bronze layer of a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        table_name (str): Name of the table to ingest data into.\n",
    "        date_columns (list or str): Name(s) of the column(s) to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are DataFrames with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Ingest Into Bronze Function.\")\n",
    "\n",
    "    # Calling the Extract Function for all CSV files\n",
    "    # Returns: raw_data_dfs, a dict where keys are table names and values are DataFrames with raw data extracted from CSV files.\n",
    "    print(\"Extract Function.\")\n",
    "    raw_data_dfs = extract(csv_folder_path)\n",
    "    if raw_data_dfs is None:\n",
    "        # Handle case where extraction fails\n",
    "        print(\"Extraction failed.\")\n",
    "        return\n",
    "\n",
    "    # Calling the Transformation Function\n",
    "    # For each table stored in raw_data_dfs, the transform_raw() perform specific date transformation for each date column.\n",
    "    # The date_columns_map is a dict where the key is the table name and the values are single date columns (in string format)\n",
    "    # or multiple columns (in list format). Recall that the transform_raw() also ensures that the date columns are always treated\n",
    "    # as a list for consistency.\n",
    "    # It returns transformed_data_dfs, where keys are table names and values are the cleaned DataFrames.\n",
    "    print(\"Transformation Function into Bronze.\")\n",
    "    transformed_data_dfs = transform_raw(raw_data_dfs, date_columns_map)\n",
    "    if transformed_data_dfs is None:\n",
    "        # Handle case where transformation fails\n",
    "        print(\"Error occurred during transformation. Processing aborted.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create the database engine\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with db_engine.connect() as connection:\n",
    "            # Check if the schema exists\n",
    "            result = connection.execute(\n",
    "                text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {\"schema\": schema_name}\n",
    "            )\n",
    "            schema_exists = result.fetchone() is not None\n",
    "            if not schema_exists:\n",
    "                raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "            # Set the search path to the specified schema\n",
    "            connection.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "            print(f\"Search path set to schema '{schema_name}'.\")\n",
    "            \n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Ingesting data into {schema_name}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp column\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Ingest data into the specified schema and table\n",
    "                # Note: if_exists='replace' performs a full refresh of the table content (drop the table and ingest last updated data).\n",
    "                # Note: use if_exists='append' is you want to append data for the specific table.\n",
    "                cleaned_data_df.to_sql(table_name, db_engine, schema=schema_name, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"CSV data ingested successfully into {schema_name}.{table_name}.\")\n",
    "                \n",
    "        return transformed_data_dfs\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration and Transformation into the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ingesting transformed data from Bronze to Silver\n",
    "def ingest_bronze_to_silver(connection_uri, bronze_schema, silver_schema, transformed_data_dfs):\n",
    "    \n",
    "    print(\"Ingest from Bronze Into Silver Function.\")\n",
    "\n",
    "    try:\n",
    "        # Create database engine\n",
    "        engine = create_db_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Check if the bronze and silver schemas exist\n",
    "            for schema_name in [bronze_schema, silver_schema]:\n",
    "                result = connection.execute(\n",
    "                    text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if not schema_exists:\n",
    "                    raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "                print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "            # Set the search path to the silver schema\n",
    "            connection.execute(text(f\"SET search_path TO {silver_schema};\"))\n",
    "            print(f\"Search path set to schema '{silver_schema}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Ingesting data into {silver_schema}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp column\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Ingest data into the specified schema and table\n",
    "                # Note: if_exists='replace' performs a full refresh of the table content (drop the table and ingest last updated data).\n",
    "                # Note: use if_exists='append' is you want to append data for the specific table.\n",
    "                cleaned_data_df.to_sql(table_name, engine, schema=silver_schema, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"Transformed data ingested successfully into {silver_schema}.{table_name}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myuser:mypassword@postgres/mydatabase\n",
      "CREATE SCHEMA\n",
      "CREATE SCHEMA\n",
      "CREATE SCHEMA\n",
      "SQL script executed successfully. Schemas were created.\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' exists in the database.\n",
      "myuser:mypassword@postgres/mydatabase\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "SQL script executed successfully. Tables were created into the Bronze Layer.\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' exists in the database.\n",
      "Ingest Into Bronze Function.\n",
      "Extract Function.\n",
      "-> CSV file 'customers.csv' loaded successfully.\n",
      "-> CSV file 'dates.csv' loaded successfully.\n",
      "-> CSV file 'products.csv' loaded successfully.\n",
      "-> CSV file 'product_usage.csv' loaded successfully.\n",
      "-> CSV file 'subscriptions.csv' loaded successfully.\n",
      "-> CSV file 'support_interactions.csv' loaded successfully.\n",
      "Transformation Function into Bronze.\n",
      "Successfully converted column 'SignupDate' to 'YYYY-MM-DD' format for table 'customers'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'Date' to 'YYYY-MM-DD' format for table 'dates'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'StartDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'EndDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' verified to exist.\n",
      "Search path set to schema 'bronze'.\n",
      "Ingesting data into bronze.customers...\n",
      "CSV data ingested successfully into bronze.customers.\n",
      "Ingesting data into bronze.dates...\n",
      "CSV data ingested successfully into bronze.dates.\n",
      "Ingesting data into bronze.products...\n",
      "CSV data ingested successfully into bronze.products.\n",
      "Ingesting data into bronze.product_usage...\n",
      "CSV data ingested successfully into bronze.product_usage.\n",
      "Ingesting data into bronze.subscriptions...\n",
      "CSV data ingested successfully into bronze.subscriptions.\n",
      "Ingesting data into bronze.support_interactions...\n",
      "CSV data ingested successfully into bronze.support_interactions.\n",
      "myuser:mypassword@postgres/mydatabase\n",
      "CREATE VIEW\n",
      "CREATE VIEW\n",
      "CREATE VIEW\n",
      "CREATE VIEW\n",
      "CREATE VIEW\n",
      "CREATE VIEW\n",
      "SQL script executed successfully. Views were created into the Silver Layer.\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' exists in the database.\n",
      "Ingest from Bronze Into Silver Function.\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' verified to exist.\n",
      "Schema 'silver' verified to exist.\n",
      "Search path set to schema 'silver'.\n",
      "Ingesting data into silver.customers...\n",
      "Transformed data ingested successfully into silver.customers.\n",
      "Ingesting data into silver.dates...\n",
      "Transformed data ingested successfully into silver.dates.\n",
      "Ingesting data into silver.products...\n",
      "Transformed data ingested successfully into silver.products.\n",
      "Ingesting data into silver.product_usage...\n",
      "Transformed data ingested successfully into silver.product_usage.\n",
      "Ingesting data into silver.subscriptions...\n",
      "Transformed data ingested successfully into silver.subscriptions.\n",
      "Ingesting data into silver.support_interactions...\n",
      "Transformed data ingested successfully into silver.support_interactions.\n"
     ]
    }
   ],
   "source": [
    "# Ingestion Parameters - Bronze Layer\n",
    "csv_folder_path = '/workspace/data/raw'\n",
    "schema_names = ['bronze'] # CHANGE HERE!\n",
    "bronze_schema = 'bronze'\n",
    "silver_schema = 'silver'\n",
    "\n",
    "# Paths to SQL scripts\n",
    "create_schemas_script_path = 'schemas/create_schemas.sql'\n",
    "create_bronze_tables_script_path = 'bronze/create_bronze_tables.sql'\n",
    "create_silver_views_script_path = 'silver/create_silver_views.sql'\n",
    "\n",
    "# Tables in Bronze\n",
    "tables_in_bronze = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "tables_in_silver = ['dim_customers', 'dim_dates', 'dim_products', 'fact_product_usage', 'fact_support_interactions','fact_subscriptions']\n",
    "\n",
    "# Define Date Columns for each Table in Bronze\n",
    "date_columns_map = {\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "# Execute functions\n",
    "\n",
    "# Run create_schemas.sql\n",
    "result = run_sql_script(create_schemas_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Schemas were created.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# Run create_bronze_tables.sql\n",
    "result = run_sql_script(create_bronze_tables_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Tables were created into the Bronze Layer.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# Ingest into Bronze\n",
    "transformed_data_dfs = ingest_csv_to_bronze(csv_folder_path, connection_uri, bronze_schema, tables_in_bronze, date_columns_map)\n",
    "\n",
    "# Run create_silver_views.sql\n",
    "result = run_sql_script(create_silver_views_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Views were created into the Silver Layer.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "ingest_bronze_to_silver(connection_uri, bronze_schema, silver_schema, transformed_data_dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
