{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "from datetime import datetime \n",
    "import os\n",
    "from subprocess import call\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text, INT, VARCHAR, DATE, TIMESTAMP, DECIMAL\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Schemas, Tables, and Views in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing SQL Scripts Against PostgreSQL (Schemas, Tables, Views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run SQL script using shell command\n",
    "# I had to pass the env parameters explicitly  to the subprocess.call() -> (PGPASSWORD, PGUSER, PGHOST, PGPORT, PGDATABASE)\n",
    "# This avoided Jupyter Notebook asking for password. \n",
    "def run_sql_script(script_name):\n",
    "    script_path = f\"/workspace/sql_scripts/{script_name}\"\n",
    "    command = f\"psql -U {user} -d {db_name} -h {host} -p {port} -f {script_path}\"\n",
    "    return call(command, shell=True, env={\n",
    "                                        'PGPASSWORD': password,\n",
    "                                        'PGUSER': user,\n",
    "                                        'PGHOST': host,\n",
    "                                        'PGPORT': port,\n",
    "                                        'PGDATABASE': db_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Schemas Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check schema existence\n",
    "def check_schema_existence(connection_uri, schema_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Schemas exist in the database ---\")\n",
    "            for schema_name in schema_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if schema_exists:\n",
    "                    print(f\"Schema '{schema_name}' exists in the database.\")\n",
    "                else:\n",
    "                    print(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(\"----- End of Schema Checking -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if Tables Exist in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check table existence\n",
    "def check_table_existence(connection_uri, schema_name, table_names):\n",
    "    try:\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "        \n",
    "        with db_engine.connect() as connection:\n",
    "            print(\"--- Checking if Tables exist ---\")\n",
    "            for table_name in table_names:\n",
    "                result = connection.execute(\n",
    "                    text(\"SELECT table_name FROM information_schema.tables WHERE table_schema = :schema AND table_name = :table\"),\n",
    "                    {\"schema\": schema_name, \"table\": table_name}\n",
    "                )\n",
    "                table_exists = result.fetchone() is not None\n",
    "                if table_exists:\n",
    "                    print(f\"Table '{table_name}' exists in schema '{schema_name}'.\")\n",
    "                else:\n",
    "                    print(f\"Table '{table_name}' does not exist in schema '{schema_name}'.\")\n",
    "            print(\"----- End of Checking Tables -----\")\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or executing query: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Tables Column Names in the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_table_columns(connection_uri, schema_name, tables_in_schema):\n",
    "    \"\"\"\n",
    "    Fetches column names for a set of tables in a specified schema from a database.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The database connection URI.\n",
    "        schema_name (str): The schema name where the tables are located.\n",
    "        tables_in_silver (list of str): A list of table names for which the column names are to be fetched.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are table names and the values are lists of column names for each table.\n",
    "    \"\"\"\n",
    "    columns_dict = {}\n",
    "    try:\n",
    "        engine = create_db_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            for table_name in tables_in_schema:\n",
    "                query = text(f\"\"\"\n",
    "                    SELECT column_name \n",
    "                    FROM information_schema.columns \n",
    "                    WHERE table_schema = '{schema_name}' \n",
    "                    AND table_name = '{table_name}';\n",
    "                \"\"\")\n",
    "                result = connection.execute(query)\n",
    "                columns = [row[0] for row in result]  # Extract the first element (column_name) and create a list columns of column_names\n",
    "                columns_dict[table_name] = columns # Fill the columns_dict with keys (table_name) and values (list of column names) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching view columns: {str(e)}\")\n",
    "\n",
    "    return columns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Bronze Tables Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bronze_table_data_types():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with data types for columns in bronze tables.\n",
    "    \"\"\"\n",
    "    bronze_data_types = {\n",
    "        'customers': {\n",
    "            'customer_id': INT,\n",
    "            'name': VARCHAR(100),\n",
    "            'age': INT,\n",
    "            'gender': VARCHAR(10),\n",
    "            'signup_date': DATE,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'subscriptions': {\n",
    "            'subscription_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'start_date': DATE,\n",
    "            'end_date': DATE,\n",
    "            'type': VARCHAR(50),\n",
    "            'status': VARCHAR(50),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'product_usage': {\n",
    "            'usage_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'date_id': INT,\n",
    "            'product_id': INT,\n",
    "            'num_logins': INT,\n",
    "            'amount': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'support_interactions': {\n",
    "            'interaction_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'date_id': INT,\n",
    "            'issue_type': VARCHAR(100),\n",
    "            'resolution_time': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'dates': {\n",
    "            'date_id': INT,\n",
    "            'date': DATE,\n",
    "            'week': INT,\n",
    "            'month': INT,\n",
    "            'quarter': INT,\n",
    "            'year': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'products': {\n",
    "            'product_id': INT,\n",
    "            'product_name': VARCHAR(100),\n",
    "            'category': VARCHAR(50),\n",
    "            'price': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        }\n",
    "    }\n",
    "    return bronze_data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Silver Table Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silver_table_data_types():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with data types for columns in silver tables.\n",
    "    \"\"\"\n",
    "    silver_data_types = {\n",
    "        'dim_customers': {\n",
    "            'customer_id': INT,\n",
    "            'name': VARCHAR(100),\n",
    "            'age': INT,\n",
    "            'gender': VARCHAR(10),\n",
    "            'signup_date': DATE,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'fact_subscriptions': {\n",
    "            'subscription_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'start_date': DATE,\n",
    "            'end_date': DATE,\n",
    "            'type': VARCHAR(50),\n",
    "            'status': VARCHAR(50),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'fact_product_usage': {\n",
    "            'usage_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'date_id': INT,\n",
    "            'product_id': INT,\n",
    "            'num_logins': INT,\n",
    "            'amount': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'fact_support_interactions': {\n",
    "            'interaction_id': INT,\n",
    "            'customer_id': INT,\n",
    "            'date_id': INT,\n",
    "            'issue_type': VARCHAR(100),\n",
    "            'resolution_time': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'dim_dates': {\n",
    "            'date_id': INT,\n",
    "            'date': DATE,\n",
    "            'week': INT,\n",
    "            'month': INT,\n",
    "            'quarter': INT,\n",
    "            'year': INT,\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        },\n",
    "        'dim_products': {\n",
    "            'product_id': INT,\n",
    "            'product_name': VARCHAR(100),\n",
    "            'category': VARCHAR(50),\n",
    "            'price': DECIMAL(10, 2),\n",
    "            'extracted_at': TIMESTAMP,\n",
    "            'inserted_at': TIMESTAMP\n",
    "        }\n",
    "    }\n",
    "    return silver_data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping columns between CSV and Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_bronze_columns(table_name):\n",
    "    \"\"\"\n",
    "    Maps column names from CSV dataframes to corresponding column names in bronze tables.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table for which column mapping is required.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names in CSV dataframes to their corresponding column names in bronze tables.\n",
    "    \"\"\"\n",
    "    if table_name == 'customers':\n",
    "        return {\n",
    "            'CustomerID': 'customer_id',\n",
    "            'Name': 'name',\n",
    "            'Age': 'age',\n",
    "            'Gender': 'gender',\n",
    "            'SignupDate': 'signup_date',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'subscriptions':\n",
    "        return {\n",
    "            'SubscriptionID': 'subscription_id',\n",
    "            'CustomerID': 'customer_id',\n",
    "            'StartDate': 'start_date',\n",
    "            'EndDate': 'end_date',\n",
    "            'Type': 'type',\n",
    "            'Status': 'status',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'product_usage':\n",
    "        return {\n",
    "            'UsageID': 'usage_id',\n",
    "            'CustomerID': 'customer_id',\n",
    "            'DateID': 'date_id',\n",
    "            'ProductID': 'product_id',\n",
    "            'NumLogins': 'num_logins',\n",
    "            'Amount': 'amount',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'support_interactions':\n",
    "        return {\n",
    "            'InteractionID': 'interaction_id',\n",
    "            'CustomerID': 'customer_id',\n",
    "            'DateID': 'date_id',\n",
    "            'IssueType': 'issue_type',\n",
    "            'ResolutionTime': 'resolution_time',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'dates':\n",
    "        return {\n",
    "            'DateID': 'date_id',\n",
    "            'Date': 'date',\n",
    "            'Week': 'week',\n",
    "            'Month': 'month',\n",
    "            'Quarter': 'quarter',\n",
    "            'Year': 'year',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    elif table_name == 'products':\n",
    "        return {\n",
    "            'ProductID': 'product_id',\n",
    "            'ProductName': 'product_name',\n",
    "            'Category': 'category',\n",
    "            'Price': 'price',\n",
    "            'extracted_at': 'extracted_at',\n",
    "            'inserted_at': 'inserted_at'\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Table '{table_name}' not found in the bronze layer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion into the Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract data from CSV files\n",
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    # Test if a folder path exists\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a list of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary where keys are table names and values are DataFrames containing data from each CSV file\n",
    "    # This allows us to iterate over all the tables and perform specific transformations in the transform_raw() function  \n",
    "    data_frames = {}\n",
    "\n",
    "    # Iterating over each CSV file in the folder\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"-> CSV file '{csv_file}' loaded successfully.\")\n",
    "            \n",
    "            # Add 'extracted_at' column with current timestamp\n",
    "            df['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Based on the CSV Files]**\n",
    "\n",
    "**Customers**\n",
    "* SignupDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "\n",
    "**Dates**\n",
    "* DateID : will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Product Usage**\n",
    "* Date column not available.\n",
    "\n",
    "**Subscriptions**\n",
    "* StartDate,EndDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "* Status: will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Support Interactions**\n",
    "* Date column not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to transform raw data\n",
    "def transform_raw(data_frames, date_columns_map):\n",
    "    \"\"\"\n",
    "    Transform multiple raw DataFrames extracted from CSV files.\n",
    "    Perform cleaning procedures: Convert specified date columns to pandas datetime and \n",
    "    boolean columns to integer.\n",
    "\n",
    "    Args:\n",
    "        data_frames (dict): A dictionary where keys are table names and values are DataFrames \n",
    "                            containing raw data extracted from CSV files.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are \n",
    "                                 column names to convert to pandas datetime.\n",
    "        boolean_columns_map (dict): (I ommited this argument as there is no boolean column at this time) A dictionary where keys are table names and values are \n",
    "                                    column names to convert from boolean to integer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are cleaned DataFrames \n",
    "              with the specified transformations applied.\n",
    "    \"\"\"\n",
    "    # Create a dictionary where keys are table names and values are the clean DataFrames after performing specific transformations.\n",
    "    cleaned_data_frames = {}\n",
    "\n",
    "    # Iterating over all tables. If table is not in the dict returned by the extract() function, then it is skipped for transformation.\n",
    "    # df contains the actual data (in the DataFrame format) for each table\n",
    "    for table_name, df in data_frames.items():\n",
    "    \n",
    "        try:\n",
    "            if table_name in date_columns_map:\n",
    "            # we specify the date columns in each table to perform transformations.\n",
    "                # If a value of a key of date_columns_map is a single date column:\n",
    "                #   Ex: date_columns == 'SignupDate', a string.\n",
    "                # If a value of a key of date_columns_map is a list of date columns:\n",
    "                #   Ex: date_columns == ['StartDate', 'EndDate'], a list.\n",
    "                date_columns = date_columns_map[table_name] # accessing the value of the key 'table_name'\n",
    "            \n",
    "            # Note: the transform_raw() can receive a list of date columns, so we need to ensure the date_columns variable\n",
    "            # is always treated as a list, even if a single date column name is provided.\n",
    "                # If date_columns is a list, the condition is True\n",
    "                # If date_columns is not a list, the condition is False, then it transforms it into a list.\n",
    "                if not isinstance(date_columns, list):\n",
    "                    date_columns = [date_columns]\n",
    "\n",
    "                # Iterate over a potential list of columns (either single or multiple), one by one making the transformation.\n",
    "                for date_column in date_columns:\n",
    "                    # Check if the date column exists in the DataFrames that correspond to the data of each table. \n",
    "                    if date_column not in df.columns:\n",
    "                        raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                    \n",
    "                    # Format the date column to 'YYYY-MM-DD' format\n",
    "                    df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "                    print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "                    print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "                \n",
    "                # Builds a DataFrame where date columns have been cleaned for each table, which is a key of this dict.\n",
    "                # Each cleaned DataFrame is stored as a value of each table.\n",
    "            \n",
    "            cleaned_data_frames[table_name] = df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            # Indicates that an error occurred during the processing of the DataFrame for table_name and it\n",
    "            # sets to None to signify that the data transformation or cleaning for that table was unsuccessful.\n",
    "            cleaned_data_frames[table_name] = None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "            cleaned_data_frames[table_name] = None\n",
    "            \n",
    "    # Returns a clean DataFrame when dates have been treated.\n",
    "    return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name, date_columns_map):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into the Bronze layer of a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are columns to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are DataFrames with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----- Ingesting Data Into Bronze Layer. -----\")\n",
    "\n",
    "    # Calling the Extract Function for all CSV files\n",
    "    print(\" -- Extract Function. --\")\n",
    "    raw_data_dfs = extract(csv_folder_path)\n",
    "    if raw_data_dfs is None:\n",
    "        print(\"Extraction failed.\")\n",
    "        return\n",
    "\n",
    "    # Calling the Transformation Function\n",
    "    print(\"-- Transformation Function. --\")\n",
    "    transformed_data_dfs = transform_raw(raw_data_dfs, date_columns_map)\n",
    "    if transformed_data_dfs is None:\n",
    "        print(\"Error occurred during transformation. Processing aborted.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Create the database engine\n",
    "        engine = create_engine(connection_uri)\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Set the search path to the specified schema\n",
    "            set_search_path_query = text(f\"SET search_path TO {schema_name};\")\n",
    "            connection.execute(set_search_path_query)\n",
    "            print(f\"Search path set to schema '{schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            print(\"-- to_sql() Ingestion Procedure in Bronze. --\")\n",
    "            for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Ingesting data into {schema_name}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp columns\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Get data types for the table from the dictionary\n",
    "                bronze_data_types = get_bronze_table_data_types()\n",
    "                data_type_dict = bronze_data_types.get(table_name)\n",
    "\n",
    "                if data_type_dict is None:\n",
    "                    raise ValueError(f\"Data types not found for table '{table_name}' in bronze layer.\")\n",
    "\n",
    "                # Ingest data into the specified schema and table with specified data types\n",
    "                cleaned_data_df.to_sql(table_name, engine, schema=schema_name, if_exists='replace', index=False, dtype=data_type_dict)\n",
    "\n",
    "                print(f\"-> CSV data ingested successfully into {schema_name}.{table_name}.\")\n",
    "\n",
    "        return transformed_data_dfs\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration and Transformation into the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_bronze_to_silver(connection_uri, bronze_schema_name, silver_schema_name, transformed_data_dfs, tables_in_silver):\n",
    "    print(\"----- Ingesting Data Into Silver Layer. -----\")\n",
    "\n",
    "    try:\n",
    "        # Create database engine\n",
    "        engine = create_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        print(\"Database engine created successfully.\")\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Check if the silver schema exists\n",
    "            schema_exists_query = text(f\"\"\"\n",
    "                SELECT schema_name \n",
    "                FROM information_schema.schemata \n",
    "                WHERE schema_name = :schema_name\n",
    "            \"\"\")\n",
    "            result = connection.execute(schema_exists_query, {\"schema_name\": silver_schema_name})\n",
    "            if result.fetchone() is None:\n",
    "                raise ValueError(f\"Schema '{silver_schema_name}' does not exist in the database.\")\n",
    "            \n",
    "            # Set the search path to the silver schema\n",
    "            connection.execute(text(f\"SET search_path TO {silver_schema_name};\"))\n",
    "            print(f\"Search path set to schema '{silver_schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database tables\n",
    "            print(\"-- Ingestion Procedure in Silver. --\")\n",
    "            for silver_table_name, (bronze_table_name, cleaned_data_df) in zip(tables_in_silver, transformed_data_dfs.items()):\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{silver_table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "\n",
    "                # Add 'extracted_at' and 'inserted_at' timestamp columns for the ingestion into Silver\n",
    "                cleaned_data_df['extracted_at'] = datetime.now()\n",
    "                cleaned_data_df['inserted_at'] = datetime.now()\n",
    "\n",
    "                # Ingest data into the specified schema and table using to_sql\n",
    "                cleaned_data_df.to_sql(silver_table_name, engine, schema=silver_schema_name, if_exists='replace', index=False, dtype=get_silver_table_data_types().get(silver_table_name))\n",
    "\n",
    "                print(f\"-> Data ingested successfully into {silver_schema_name}.{silver_table_name}.\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"SQLAlchemyError occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Creating SCHEMAS in PostgreSQL -----\n",
      "CREATE SCHEMA\n",
      "CREATE SCHEMA\n",
      "CREATE SCHEMA\n",
      "SQL script executed successfully. Schemas were created.\n",
      "Database engine created successfully.\n",
      "--- Checking if Schemas exist in the database ---\n",
      "Schema 'bronze' exists in the database.\n",
      "Schema 'silver' exists in the database.\n",
      "Schema 'gold' exists in the database.\n",
      "----- End of Schema Checking -----\n",
      "----- Creating TABLES in Bronze -----\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "SQL script executed successfully. Tables were created in the Bronze Layer.\n",
      "Database engine created successfully.\n",
      "--- Checking if Tables exist ---\n",
      "Table 'customers' exists in schema 'bronze'.\n",
      "Table 'dates' exists in schema 'bronze'.\n",
      "Table 'product_usage' exists in schema 'bronze'.\n",
      "Table 'products' exists in schema 'bronze'.\n",
      "Table 'subscriptions' exists in schema 'bronze'.\n",
      "Table 'support_interactions' exists in schema 'bronze'.\n",
      "----- End of Checking Tables -----\n",
      "----- Ingesting Data Into Bronze Layer. -----\n",
      " -- Extract Function. --\n",
      "-> CSV file 'customers.csv' loaded successfully.\n",
      "-> CSV file 'dates.csv' loaded successfully.\n",
      "-> CSV file 'products.csv' loaded successfully.\n",
      "-> CSV file 'product_usage.csv' loaded successfully.\n",
      "-> CSV file 'subscriptions.csv' loaded successfully.\n",
      "-> CSV file 'support_interactions.csv' loaded successfully.\n",
      "-- Transformation Function. --\n",
      "Successfully converted column 'SignupDate' to 'YYYY-MM-DD' format for table 'customers'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'Date' to 'YYYY-MM-DD' format for table 'dates'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'StartDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'EndDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Search path set to schema 'bronze'.\n",
      "-- to_sql() Ingestion Procedure in Bronze. --\n",
      "Ingesting data into bronze.customers...\n",
      "-> CSV data ingested successfully into bronze.customers.\n",
      "Ingesting data into bronze.dates...\n",
      "-> CSV data ingested successfully into bronze.dates.\n",
      "Ingesting data into bronze.products...\n",
      "-> CSV data ingested successfully into bronze.products.\n",
      "Ingesting data into bronze.product_usage...\n",
      "-> CSV data ingested successfully into bronze.product_usage.\n",
      "Ingesting data into bronze.subscriptions...\n",
      "-> CSV data ingested successfully into bronze.subscriptions.\n",
      "Ingesting data into bronze.support_interactions...\n",
      "-> CSV data ingested successfully into bronze.support_interactions.\n",
      "Database engine created successfully.\n",
      "{'customers': ['CustomerID', 'Name', 'Age', 'Gender', 'SignupDate', 'extracted_at', 'inserted_at'], 'dates': ['DateID', 'Date', 'Week', 'Month', 'Quarter', 'Year', 'extracted_at', 'inserted_at'], 'product_usage': ['UsageID', 'CustomerID', 'DateID', 'ProductID', 'NumLogins', 'Amount', 'extracted_at', 'inserted_at'], 'products': ['ProductID', 'ProductName', 'Category', 'Price', 'extracted_at', 'inserted_at'], 'subscriptions': ['SubscriptionID', 'CustomerID', 'StartDate', 'EndDate', 'Type', 'Status', 'extracted_at', 'inserted_at'], 'support_interactions': ['InteractionID', 'CustomerID', 'DateID', 'IssueType', 'ResolutionTime', 'extracted_at', 'inserted_at']}\n",
      "----- Creating TABLES in Silver -----\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "CREATE TABLE\n",
      "SQL script executed successfully. Tables were created in the Silver Layer.\n",
      "Database engine created successfully.\n",
      "--- Checking if Tables exist ---\n",
      "Table 'dim_customers' exists in schema 'silver'.\n",
      "Table 'dim_dates' exists in schema 'silver'.\n",
      "Table 'fact_product_usage' exists in schema 'silver'.\n",
      "Table 'dim_products' exists in schema 'silver'.\n",
      "Table 'fact_subscriptions' exists in schema 'silver'.\n",
      "Table 'fact_support_interactions' exists in schema 'silver'.\n",
      "----- End of Checking Tables -----\n",
      "----- Ingesting Data Into Silver Layer. -----\n",
      "Database engine created successfully.\n",
      "Search path set to schema 'silver'.\n",
      "-- Ingestion Procedure in Silver. --\n",
      "-> Data ingested successfully into silver.dim_customers.\n",
      "-> Data ingested successfully into silver.dim_dates.\n",
      "-> Data ingested successfully into silver.fact_product_usage.\n",
      "-> Data ingested successfully into silver.dim_products.\n",
      "-> Data ingested successfully into silver.fact_subscriptions.\n",
      "-> Data ingested successfully into silver.fact_support_interactions.\n"
     ]
    }
   ],
   "source": [
    "# Ingestion Parameters - Bronze Layer\n",
    "csv_folder_path = '/workspace/data/raw'\n",
    "schema_names = ['bronze', 'silver', 'gold']\n",
    "bronze_schema = 'bronze'\n",
    "silver_schema = 'silver'\n",
    "\n",
    "# Paths to SQL scripts\n",
    "create_schemas_script_path = 'schemas/create_schemas.sql'\n",
    "create_bronze_tables_script_path = 'bronze/create_bronze_tables.sql'\n",
    "create_silver_tables_script_path = 'silver/create_silver_tables.sql'\n",
    "\n",
    "# Tables in Bronze\n",
    "tables_in_bronze = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "tables_in_silver = ['dim_customers', 'dim_dates', 'fact_product_usage', 'dim_products', 'fact_subscriptions', 'fact_support_interactions']\n",
    "\n",
    "# Define Date Columns for each Table in Bronze\n",
    "date_columns_map = {\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "# Execute functions\n",
    "\n",
    "# Run create_schemas.sql\n",
    "print(\"----- Creating SCHEMAS in PostgreSQL -----\")\n",
    "result = run_sql_script(create_schemas_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Schemas were created.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_schema_existence(connection_uri, schema_names)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# Run create_bronze_tables.sql\n",
    "print(\"----- Creating TABLES in Bronze -----\")\n",
    "result = run_sql_script(create_bronze_tables_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Tables were created in the Bronze Layer.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_table_existence(connection_uri, bronze_schema, tables_in_bronze)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# Data Ingestion into Bronze with Minor Transformation\n",
    "transformed_data_dfs = ingest_csv_to_bronze(csv_folder_path, connection_uri, bronze_schema, date_columns_map)\n",
    "# for names, dfs in transformed_data_dfs.items():\n",
    "#     print(dfs.head(1))\n",
    "\n",
    "columns_dict = get_schema_table_columns(connection_uri, bronze_schema, tables_in_bronze)\n",
    "print(columns_dict)\n",
    "\n",
    "\n",
    "# Run create_silver_tables.sql\n",
    "print(\"----- Creating TABLES in Silver -----\")\n",
    "result = run_sql_script(create_silver_tables_script_path)\n",
    "if result == 0:\n",
    "    print(\"SQL script executed successfully. Tables were created in the Silver Layer.\")\n",
    "    # Check if schemas exist in the database\n",
    "    check_table_existence(connection_uri, silver_schema, tables_in_silver)\n",
    "else:\n",
    "    print(\"Error executing SQL script.\")\n",
    "\n",
    "# Data Integration and Transformation into Silver\n",
    "ingest_bronze_to_silver(connection_uri, bronze_schema, silver_schema, transformed_data_dfs, tables_in_silver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
