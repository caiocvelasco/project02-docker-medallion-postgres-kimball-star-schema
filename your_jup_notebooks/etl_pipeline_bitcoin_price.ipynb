{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "import requests     # Establishing connection with the web (API)\n",
    "import json         # Exposes an API familiar to users of the standard library marshal and pickle modules\n",
    "import pytest       # Makes it easy to write small, readable tests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import Engine, create_engine\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [E] Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API\n",
    "- Data Source: https://docs.coincap.io/#89deffa0-ab03-4e0a-8d92-637a857d2c91\n",
    "- Data comes in a JSON format (semi-structured data) from a 3rd Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(api_get_request_url):\n",
    "    \"\"\"\n",
    "    Create and return a raw JSON extracted from an external API via GET Request.\n",
    "\n",
    "    Args:\n",
    "        api_get_request_url: The URL for API's GET Request.\n",
    "\n",
    "    Returns:\n",
    "        raw_data_json: a raw JSON with the API data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # GET Request. Package the request, send the request and catch the response r\n",
    "    r = requests.get(api_get_request_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if r.status_code == 200:\n",
    "        # Extract JSON data from response: Decode the JSON data into a dictionary\n",
    "        raw_data_json = r.json()\n",
    "    else:\n",
    "        print(f\"Error. Non-success status code: {r.status_code}\")\n",
    "    \n",
    "    # Return the json\n",
    "    return raw_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start UNIX time in milliseconds: 1672574400000\n",
      "End UNIX time in milliseconds: 1717156800000\n",
      "This is the time series URL for bitcoin:\n",
      "https://api.coincap.io/v2/assets/bitcoin/history?interval=d1&start=1672574400000&end=1717156800000\n"
     ]
    }
   ],
   "source": [
    "# Function to Convert Dates to UNIX Milliseconds\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_to_unix_milliseconds(start_date_str, end_date_str, date_format='%Y-%m-%d %H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Converts start and end dates to UNIX time in milliseconds.\n",
    "\n",
    "    Parameters:\n",
    "    start_date_str (str): The start date as a string.\n",
    "    end_date_str (str): The end date as a string.\n",
    "    date_format (str): The format of the input date strings (default is '%Y-%m-%d %H:%M:%S').\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the start and end dates in UNIX milliseconds.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the date strings to datetime objects\n",
    "        start_date = datetime.strptime(start_date_str, date_format)\n",
    "        end_date = datetime.strptime(end_date_str, date_format)\n",
    "\n",
    "        # Convert datetime objects to UNIX time in seconds and then to milliseconds\n",
    "        start_unix_ms = int(start_date.timestamp() * 1000)\n",
    "        end_unix_ms = int(end_date.timestamp() * 1000)\n",
    "\n",
    "        return start_unix_ms, end_unix_ms\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Calling the function:\n",
    "start_date_str = '2023-01-01 12:00:00'\n",
    "end_date_str = '2024-05-31 12:00:00'\n",
    "start_unix_ms, end_unix_ms = convert_to_unix_milliseconds(start_date_str, end_date_str)\n",
    "print(f\"Start UNIX time in milliseconds: {start_unix_ms}\")\n",
    "print(f\"End UNIX time in milliseconds: {end_unix_ms}\")\n",
    "\n",
    "# Building the GET Request URL\n",
    "coin_str = 'bitcoin'\n",
    "interval_str = 'd1'\n",
    "start_date_str = start_unix_ms\n",
    "end_date_str = end_unix_ms\n",
    "print(f\"This is the time series URL for {coin_str}:\")\n",
    "bitcoin_url = f\"https://api.coincap.io/v2/assets/{coin_str}/history?interval={interval_str}&start={start_date_str}&end={end_date_str}\"\n",
    "print(bitcoin_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the extract() function on a time series data\n",
    "# d1 = the day interval represents average of 24 hour periods (https://docs.coincap.io/#89deffa0-ab03-4e0a-8d92-637a857d2c91)\n",
    "raw_data_json = extract(bitcoin_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing JSON names (keys):\n",
      "-> data\n",
      "-> timestamp\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74cda394c1c41bab6fd5d7f12f96748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='{\\n  \"data\": [\\n    {\\n      \"priceUsd\": \"16708.5235619029337193\",\\n      \"time\": 167261760000â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick visualization\n",
    "print(\"Printing JSON names (keys):\")\n",
    "for key in raw_data_json.keys():\n",
    "    print(\"->\", key)\n",
    "\n",
    "# Convert JSON to string\n",
    "json_str = json.dumps(raw_data_json, indent=2)\n",
    "\n",
    "# Create a text area widget\n",
    "json_output = widgets.Textarea(\n",
    "    value=json_str,\n",
    "    placeholder='JSON data',\n",
    "    description='JSON:',\n",
    "    disabled=True,\n",
    "    layout=widgets.Layout(width='80%', height='200px')\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [T] Transformation\n",
    "* Exploring JSON Structure\n",
    "* Data Normalization\n",
    "    * Transforming the JSON into a table (as a Dataframe)\n",
    "* Data Exploration & Cleaning\n",
    "    * Check for: Missing Data, Data Types, Rounding needs.\n",
    "* Transforming into a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring JSON Structure\n",
    "* Exploring the JSON name-value pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Json's type from the API request: <class 'dict'>\n",
      "It's a dict, so let's check how many keys it has: 2\n",
      "It is a nested JSON, with two key-value pais: data and timestamp.\n",
      "The actual information comes in the 'data' key along with its associated 'timestamp' as another key.\n",
      "How many key-value pairs in the 'data' key: 516\n",
      "-> There are 3 features in a single record of 'data'.\n",
      "-> In total, there are 516 records stored in 'data' and each record has 3 features.\n",
      "We should see the same when storing in Dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Exploring the JSON structure\n",
    "print(\"This is the Json's type from the API request:\", type(raw_data_json))\n",
    "print(\"It's a dict, so let's check how many keys it has:\", len(raw_data_json))\n",
    "\n",
    "# Saving Keys and Values as Lists\n",
    "raw_data_json_keys_ls = list(raw_data_json.keys())\n",
    "raw_data_json_values_ls = list(raw_data_json.values())\n",
    "\n",
    "print(\"It is a nested JSON, with two key-value pais: data and timestamp.\")\n",
    "print(\"The actual information comes in the 'data' key along with its associated 'timestamp' as another key.\")\n",
    "print(\"How many key-value pairs in the 'data' key:\", len(raw_data_json_values_ls[0]))\n",
    "potential_columns = len(raw_data_json_values_ls[0][0])\n",
    "potential_rows = len(raw_data_json_values_ls[0])\n",
    "print(f\"-> There are {potential_columns} features in a single record of 'data'.\")\n",
    "print(f\"-> In total, there are {potential_rows} records stored in 'data' and each record has {potential_columns} features.\")\n",
    "print(\"We should see the same when storing in Dataframe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Function\n",
    "Notes: \n",
    "* The list of records are stored in the 'data' key of the JSON file, as seen above.\n",
    "* Before creating the transformation function, we performed a data exploration:\n",
    "    * The price data was converted to Numerical (float) just in case.\n",
    "    * The date seems to follow the ISO 8601 format. Thus, it was converted to Pandas `datetime`.\n",
    "    * There were some **Missing** (`NULL`) data ('maxSupply', 'explorer'). We handled it by hardcoding within the transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Converting part\n",
    "# Selecting the columns and their correct types\n",
    "cols_datatypes = {\n",
    "    'priceUsd' : float\n",
    "}\n",
    "\n",
    "# For the Rounding part\n",
    "# Selecting the columns we want to round to some decimal places\n",
    "cols_to_round = ['priceUsd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data_json, cols_datatypes, cols_to_round):\n",
    "    \"\"\"\n",
    "    Create and return a raw dataset in the DataFrame format based on a JSON.\n",
    "    Here, the function is not automated to both the missing values and the rounding parts.\n",
    "    You need to hardcode it here how to handle each case.\n",
    "\n",
    "    Args:\n",
    "        raw_data_json: The raw json returned by the extraction function.\n",
    "        cols_datatypes: A dictionary of columns and the associated types you want for them.\n",
    "    Returns:\n",
    "        raw_normalized_data_df: a raw, normalized dataset in the DataFrame format.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # 1) Data Normalization\n",
    "    # record_path = \"data\": path to list of records.\n",
    "    # sep = \".\": Nested records will generate names separated by 'sep'.\n",
    "    raw_normalized_data_df = pd.json_normalize(raw_data_json, record_path = \"data\", sep = '.') \n",
    "\n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(raw_normalized_data_df, pd.DataFrame)\n",
    "    \n",
    "    # 2) Data Cleaning\n",
    "    # Inner Function to Convert Categorical to Numerical\n",
    "    def convert_cols_cat_to_num(df, cols_datatypes):\n",
    "        \"\"\"\n",
    "        Gets a dataframe with specific categorical columns and convert them into numerical columns. \n",
    "\n",
    "        Args:\n",
    "            df: The dataframe with the columns you want to convert.\n",
    "            cols_datatypes: A dictionary of columns and the associated types you want for them. \n",
    "\n",
    "        Returns:\n",
    "            Engine: A dataframe with columns converted to the types you want. \n",
    "        \"\"\"\n",
    "        for col, datatype in cols_datatypes.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(datatype)\n",
    "        return df\n",
    "    \n",
    "    # Converting Categorical to Numerical\n",
    "    converted_data_df = convert_cols_cat_to_num(raw_normalized_data_df, cols_datatypes)\n",
    "\n",
    "    # Inner function to convert ISO 8601 date to pandas datetime\n",
    "    def convert_iso_date_to_datetime(df, col_name):\n",
    "        try:\n",
    "            # Convert the specified column to pandas datetime \n",
    "            df[col_name] = pd.to_datetime(df[col_name]).dt.date\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when converting the date: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Convert Date to Pandas Datetime\n",
    "    converted_data_df = convert_iso_date_to_datetime(converted_data_df, 'date')\n",
    "    \n",
    "    # Inner function to drop columns\n",
    "    def drop_columns(df, cols_to_drop):\n",
    "        try:\n",
    "            # Drop specified columns\n",
    "            df.drop(columns=cols_to_drop, inplace=True)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when dropping columns: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Dropping 'time' column\n",
    "    converted_data_df = drop_columns(converted_data_df, ['time'])\n",
    "\n",
    "    # Handling Missing Values\n",
    "    # There is no non-null values, so we are okay.    \n",
    "\n",
    "    # Inner function to round columns\n",
    "    def round_to_two_decimal_places(df, cols_to_round):\n",
    "        try:\n",
    "            # Round columns to 2 decimal places\n",
    "            # Note: 'applymap' applies a function that accepts and returns a scalar to every element of a DataFrame\n",
    "            df[cols_to_round] = df[cols_to_round].apply(lambda x: x.round(2))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when rounding: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Rounding columns\n",
    "    cleaned_data_df = round_to_two_decimal_places(converted_data_df, cols_to_round)\n",
    "    \n",
    "    return cleaned_data_df\n",
    "\n",
    "# Try-except block for the transform function\n",
    "try:\n",
    "    # Transform the raw_data_json into a cleaned and tabular dataframe\n",
    "    cleaned_data_df = transform(raw_data_json, cols_datatypes, cols_to_round)\n",
    "except:\n",
    "    print(\"There is an error with the transform function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 516 entries, 0 to 515\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   priceUsd  516 non-null    float64\n",
      " 1   date      516 non-null    object \n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Checking the cleaned dataset\n",
    "cleaned_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceUsd</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16708.52</td>\n",
       "      <td>2023-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16682.71</td>\n",
       "      <td>2023-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16822.19</td>\n",
       "      <td>2023-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16826.73</td>\n",
       "      <td>2023-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16821.14</td>\n",
       "      <td>2023-01-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   priceUsd        date\n",
       "0  16708.52  2023-01-02\n",
       "1  16682.71  2023-01-03\n",
       "2  16822.19  2023-01-04\n",
       "3  16826.73  2023-01-05\n",
       "4  16821.14  2023-01-06"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first records\n",
    "cleaned_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transforming into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform a Python Dataframe into a CSV file \n",
    "\n",
    "def df_to_csv(df, file_name):\n",
    "    \"\"\"\n",
    "    Transforms a pandas DataFrame into a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be transformed.\n",
    "    file_name (str): The name of the CSV file to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_name, index=False)\n",
    "        print(f\"DataFrame successfully saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the DataFrame to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to /workspace/sources/bitcoin_time_series.csv\n"
     ]
    }
   ],
   "source": [
    "# Calling the function for the Crypto dataset\n",
    "df_to_csv(cleaned_data_df, '/workspace/sources/bitcoin_time_series.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L] Load\n",
    "* Loading data to Postgres.\n",
    "    * Open a SQL connection with SQLAlchemy\n",
    "    * .to_sql()\n",
    "* Data Quality checks:\n",
    "    * Validate that data was correctly persisted in postgres\n",
    "        * Ensure it can be queried\n",
    "            * pd.read_sql()\n",
    "        * Make sure counts match\n",
    "        * Validate each row is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Function that created the SQL Engine based on SQLAlchemy\n",
    "def create_db_engine(connection_uri: str) -> Engine: # Arrow indicates that the function returns an object of type Engine\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    db_engine = create_engine(connection_uri)\n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")\n",
    "\n",
    "# Create the database engine\n",
    "db_engine = create_db_engine(connection_uri)\n",
    "\n",
    "# Close the engine connection\n",
    "db_engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/Persist data in Postgres\n",
    "def load(cleaned_data_df, con_engine):\n",
    "    \"\"\"\n",
    "    Load/persist data in PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        raw_normalized_data_df (DataFrame): The DataFrame containing the data to be loaded.\n",
    "        con_engine (Engine): SQLAlchemy engine for database connection.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # to_sql: Write records stored in a DataFrame to a SQL database.\n",
    "    cleaned_data_df.to_sql(name=\"bitcoin_price\", con=con_engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "* In the Load function, we are basically checking if a table named 'crypto_mkt' exists. If it exists, the `'if_exists' = 'replace'`  argument will drop the table before inserting new values.\n",
    "* If you want something different, please check the [to_sql()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   priceUsd        date\n",
      "0  16708.52  2023-01-02\n",
      "1  16682.71  2023-01-03\n",
      "2  16822.19  2023-01-04\n",
      "3  16826.73  2023-01-05\n",
      "4  16821.14  2023-01-06\n"
     ]
    }
   ],
   "source": [
    "# Call the load function to load the transformed data to persistent storage\n",
    "load(cleaned_data_df, db_engine)\n",
    "\n",
    "# Query the data in the crypto_mkt table, check the head of the DataFrame\n",
    "to_validate = pd.read_sql(\"SELECT * FROM bitcoin_price\", con=db_engine)\n",
    "print(to_validate.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
