{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "import pytest       # Makes it easy to write small, readable tests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [L] Load\n",
    "* Loading data to Postgres.\n",
    "    * Open a SQL connection with SQLAlchemy\n",
    "    * .to_sql()\n",
    "* Data Quality checks:\n",
    "    * Validate that data was correctly persisted in postgres\n",
    "        * Ensure it can be queried\n",
    "            * pd.read_sql()\n",
    "        * Make sure counts match\n",
    "        * Validate each row is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Function that created the SQL Engine based on SQLAlchemy\n",
    "def create_db_engine(connection_uri: str) -> Engine: # Arrow indicates that the function returns an object of type Engine\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    db_engine = create_engine(connection_uri)\n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")\n",
    "\n",
    "# # Create the database engine\n",
    "# db_engine = create_db_engine(connection_uri)\n",
    "\n",
    "# # Close the engine connection\n",
    "# db_engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully.\n",
      "Database engine created successfully.\n",
      "Schema 'churn_bronze' verified to exist.\n",
      "Dependent views dropped successfully.\n",
      "Search path set to schema 'churn_bronze'.\n",
      "Error occurred while connecting to the database or ingesting data: (psycopg2.errors.DependentObjectsStillExist) cannot drop table churn_bronze.customer_data because other objects depend on it\n",
      "DETAIL:  view churn_silver.customer_summary depends on table churn_bronze.customer_data\n",
      "view churn_gold.aggregated_summary depends on view churn_silver.customer_summary\n",
      "HINT:  Use DROP ... CASCADE to drop the dependent objects too.\n",
      "\n",
      "[SQL: \n",
      "DROP TABLE churn_bronze.customer_data]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n"
     ]
    }
   ],
   "source": [
    "def ingest_csv_to_bronze(csv_file, db_url, schema_name, table_name):\n",
    "    try:\n",
    "        # Load CSV data into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(\"CSV file loaded successfully.\")\n",
    "\n",
    "        # Create a SQLAlchemy engine to connect to the PostgreSQL database\n",
    "        engine = create_engine(db_url)\n",
    "        print(\"Database engine created successfully.\")\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Check if the schema exists\n",
    "            result = connection.execute(\n",
    "                text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {\"schema\": schema_name}\n",
    "            )\n",
    "            schema_exists = result.fetchone() is not None\n",
    "            if not schema_exists:\n",
    "                raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "            # Drop dependent views\n",
    "            connection.execute(text(\"DROP VIEW IF EXISTS churn_gold.aggregated_summary CASCADE;\"))\n",
    "            connection.execute(text(\"DROP VIEW IF EXISTS churn_silver.customer_summary CASCADE;\"))\n",
    "            print(\"Dependent views dropped successfully.\")\n",
    "\n",
    "            # Set the search path to the specified schema\n",
    "            connection.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "            print(f\"Search path set to schema '{schema_name}'.\")\n",
    "\n",
    "        # Write the DataFrame to the PostgreSQL table in the specified schema\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False, schema=schema_name)\n",
    "        print(f\"CSV data ingested successfully into {schema_name}.{table_name}.\")\n",
    "\n",
    "        # Recreate dependent views\n",
    "        # connection.execute(text(\"\"\"\n",
    "        #     CREATE VIEW churn_silver.customer_summary AS\n",
    "        #     SELECT\n",
    "        #         customer_id,\n",
    "        #         age,\n",
    "        #         gender,\n",
    "        #         total_transactions,\n",
    "        #         last_purchase_date,\n",
    "        #         churn_status,\n",
    "        #         CASE\n",
    "        #             WHEN total_transactions >= 100 AND last_purchase_date >= '2023-01-01' THEN 'active'\n",
    "        #             ELSE 'inactive'\n",
    "        #         END AS customer_status\n",
    "        #     FROM churn_bronze.customer_data;\n",
    "        # \"\"\"))\n",
    "        # connection.execute(text(\"\"\"\n",
    "        #     CREATE VIEW churn_gold.aggregated_summary AS\n",
    "        #     SELECT\n",
    "        #         customer_status,\n",
    "        #         COUNT(*) AS customer_count,\n",
    "        #         AVG(age) AS avg_age,\n",
    "        #         AVG(total_transactions) AS avg_transactions\n",
    "        #     FROM churn_silver.customer_summary\n",
    "        #     GROUP BY customer_status;\n",
    "        # \"\"\"))\n",
    "        # print(\"Dependent views recreated successfully.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(str(ve))\n",
    "\n",
    "# Usage example:\n",
    "csv_file_path = '/workspace/data/customer_churn.csv'\n",
    "schema_name = 'churn_bronze'\n",
    "table_name = 'customer_data'\n",
    "\n",
    "ingest_csv_to_bronze(csv_file_path, connection_uri, schema_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "* In the Load function, we are basically checking if a table named 'crypto_mkt' exists. If it exists, the `'if_exists' = 'replace'`  argument will drop the table before inserting new values.\n",
    "* If you want something different, please check the [to_sql()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
