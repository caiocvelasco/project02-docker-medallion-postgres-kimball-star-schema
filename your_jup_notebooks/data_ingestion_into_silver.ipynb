{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PostgreSQL Connection Engine with SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create an SQLAlchemy engine\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute create_silver_views.sql\n",
    "def create_silver_views(connection_uri, silver_schema, script_path):\n",
    "    try:\n",
    "        with open(script_path, 'r') as file:\n",
    "            sql_script = file.read()\n",
    "        \n",
    "        engine = create_db_engine(connection_uri)\n",
    "        with engine.connect() as connection:\n",
    "            # Set the search path to the silver schema\n",
    "            connection.execute(text(f\"SET search_path TO {silver_schema};\"))\n",
    "            # Execute the script to create views\n",
    "            connection.execute(text(sql_script))\n",
    "            \n",
    "        print(\"Silver views created successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"create_silver_views.sql not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing create_silver_views.sql: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Based on the CSV Files]**\n",
    "\n",
    "**table**\n",
    "* xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define function to transform raw data\n",
    "# def transform_raw(data_frames, date_columns_map):\n",
    "#     \"\"\"\n",
    "#     Transform multiple raw DataFrames extracted from CSV files.\n",
    "#     Perform cleaning procedures: Convert specified date columns to pandas datetime and \n",
    "#     boolean columns to integer.\n",
    "\n",
    "#     Args:\n",
    "#         data_frames (dict): A dictionary where keys are table names and values are DataFrames \n",
    "#                             containing raw data extracted from CSV files.\n",
    "#         date_columns_map (dict): A dictionary where keys are table names and values are \n",
    "#                                  column names to convert to pandas datetime.\n",
    "#         boolean_columns_map (dict): (I ommited this argument as there is no boolean column at this time) A dictionary where keys are table names and values are \n",
    "#                                     column names to convert from boolean to integer.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary where keys are table names and values are cleaned DataFrames \n",
    "#               with the specified transformations applied.\n",
    "#     \"\"\"\n",
    "#     # Create a dictionary where keys are table names and values are the clean DataFrames after performing specific transformations.\n",
    "#     cleaned_data_frames = {}\n",
    "\n",
    "#     # Iterating over all tables. If table is not in the dict returned by the extract() function, then it is skipped for transformation.\n",
    "#     # df contains the actual data (in the DataFrame format) for each table\n",
    "#     for table_name, df in data_frames.items():\n",
    "    \n",
    "#         try:\n",
    "#             if table_name in date_columns_map:\n",
    "#             # we specify the date columns in each table to perform transformations.\n",
    "#                 # If a value of a key of date_columns_map is a single date column:\n",
    "#                 #   Ex: date_columns == 'SignupDate', a string.\n",
    "#                 # If a value of a key of date_columns_map is a list of date columns:\n",
    "#                 #   Ex: date_columns == ['StartDate', 'EndDate'], a list.\n",
    "#                 date_columns = date_columns_map[table_name] # accessing the value of the key 'table_name'\n",
    "            \n",
    "#             # Note: the transform_raw() can receive a list of date columns, so we need to ensure the date_columns variable\n",
    "#             # is always treated as a list, even if a single date column name is provided.\n",
    "#                 # If date_columns is a list, the condition is True\n",
    "#                 # If date_columns is not a list, the condition is False, then it transforms it into a list.\n",
    "#                 if not isinstance(date_columns, list):\n",
    "#                     date_columns = [date_columns]\n",
    "\n",
    "#                 # Iterate over a potential list of columns (either single or multiple), one by one making the transformation.\n",
    "#                 for date_column in date_columns:\n",
    "#                     # Check if the date column exists in the DataFrames that correspond to the data of each table. \n",
    "#                     if date_column not in df.columns:\n",
    "#                         raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                    \n",
    "#                     # Format the date column to 'YYYY-MM-DD' format\n",
    "#                     df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "#                     print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "#                     print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "                \n",
    "#                 # Builds a DataFrame where date columns have been cleaned for each table, which is a key of this dict.\n",
    "#                 # Each cleaned DataFrame is stored as a value of each table.\n",
    "            \n",
    "#             cleaned_data_frames[table_name] = df\n",
    "\n",
    "#         except ValueError as ve:\n",
    "#             print(ve)\n",
    "#             # Indicates that an error occurred during the processing of the DataFrame for table_name and it\n",
    "#             # sets to None to signify that the data transformation or cleaning for that table was unsuccessful.\n",
    "#             cleaned_data_frames[table_name] = None\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "#             cleaned_data_frames[table_name] = None\n",
    "            \n",
    "#     # Returns a clean DataFrame when dates have been treated.\n",
    "#     return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ingesting from Bronze to Silver\n",
    "def ingest_bronze_to_silver(connection_uri, bronze_schema, silver_schema, transformed_data_dfs):\n",
    "    try:\n",
    "        # Create database engine\n",
    "        engine = create_db_engine(connection_uri)\n",
    "        if engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        # Verify connection and schema existence\n",
    "        with engine.connect() as connection:\n",
    "            # Check if the bronze and silver schemas exist\n",
    "            for schema_name in [bronze_schema, silver_schema]:\n",
    "                result = connection.execute(\n",
    "                    text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                    {\"schema\": schema_name}\n",
    "                )\n",
    "                schema_exists = result.fetchone() is not None\n",
    "                if not schema_exists:\n",
    "                    raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "                print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "            # Set the search path to the silver schema\n",
    "            connection.execute(text(f\"SET search_path TO {silver_schema};\"))\n",
    "            print(f\"Search path set to schema '{silver_schema}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "                if cleaned_data_df is None:\n",
    "                    print(f\"Skipping ingestion for table '{table_name}' due to previous errors.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Ingesting data into {silver_schema}.{table_name}...\")\n",
    "\n",
    "                # Add 'inserted_at' timestamp column\n",
    "                cleaned_data_df['inserted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                # Ingest data into the specified schema and table\n",
    "                # Note: if_exists='replace' performs a full refresh of the table content (drop the table and ingest last updated data).\n",
    "                # Note: use if_exists='append' is you want to append data for the specific table.\n",
    "                cleaned_data_df.to_sql(table_name, engine, schema=silver_schema, if_exists='replace', index=False)\n",
    "\n",
    "                print(f\"Transformed data ingested successfully into {silver_schema}.{table_name}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Ingestion parameters\n",
    "csv_folder_path = '/workspace/data/raw'\n",
    "schema_name_bronze = 'bronze'\n",
    "schema_name_silver = 'silver'\n",
    "\n",
    "# Tables in each schema\n",
    "tables_in_bronze = ['customers', 'dates', 'product_usage', 'products', 'subscriptions', 'support_interactions']\n",
    "tables_in_silver = ['dim_customers', 'dim_dates', 'dim_products']\n",
    "# tables_in_gold = ['customer_segmentation', 'churn_prediction']\n",
    "\n",
    "# Define date columns for each table\n",
    "date_columns_map = {\n",
    "    'customers': 'SignupDate',\n",
    "    'dates': 'Date',\n",
    "    'subscriptions': ['StartDate', 'EndDate'],\n",
    "}\n",
    "\n",
    "# Ingest data into Bronze layer\n",
    "ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name_bronze, tables_in_bronze, date_columns_map)\n",
    "# for table_name in tables_in_bronze:\n",
    "#     print(f\"Ingesting '{table_name}' into {schema_name_bronze}...\")\n",
    "#     ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name_bronze, table_name, date_columns_map[table_name])\n",
    "\n",
    "# Ingest data into Silver layer\n",
    "# for table_name in tables_in_silver:\n",
    "#     print(f\"Ingesting '{table_name}' from {schema_name_bronze} to {schema_name_silver}...\")\n",
    "#     ingest_bronze_to_silver(connection_uri, schema_name_bronze, schema_name_silver, date_columns_map[table_name])\n",
    "\n",
    "# Ingest data into Gold layer\n",
    "# for table_name in tables_in_gold:\n",
    "#     print(f\"Ingesting '{table_name}' from {schema_name_silver} to {schema_name_gold}...\")\n",
    "#     ingest_silver_to_gold(connection_uri, schema_name_silver, schema_name_gold, date_columns_map[table_name])\n",
    "\n",
    "print(\"All data ingested successfully into the Bronze Layer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
