{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Data Transformation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(csv_folder_path):\n",
    "    \"\"\"\n",
    "    Extract data from all CSV files in a folder, one by one.\n",
    "    \n",
    "    Args:\n",
    "    - csv_folder_path (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are table names and values are DataFrames containing data from each CSV file.\n",
    "    \"\"\"\n",
    "    print(\"Extract Function.\")\n",
    "    if not os.path.exists(csv_folder_path):\n",
    "        print(f\"Folder '{csv_folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # List of CSV files in the designated folder\n",
    "    csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in folder '{csv_folder_path}'.\")\n",
    "        return None\n",
    "    \n",
    "    data_frames = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Separate the file name from the extension and store it\n",
    "        table_name = os.path.splitext(csv_file)[0]  # Assuming table name is CSV filename without extension\n",
    "        # Join CSV folder path with the CSV file name, inserting '/' as needed\n",
    "        file_path = os.path.join(csv_folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"CSV file '{csv_file}' loaded successfully.\")\n",
    "            # Store the CSV in DataFrame format as a value of the dictionary's key\n",
    "            data_frames[table_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file '{csv_file}': {str(e)}\")\n",
    "            data_frames[table_name] = None\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Extraction\n",
    "# csv_folder_path = '/workspace/data/raw'\n",
    "# raw_data_dfs = extract(csv_folder_path)\n",
    "\n",
    "# if raw_data_dfs is None:\n",
    "#     # Handle case where extraction fails\n",
    "#     print(\"Extraction failed.\")\n",
    "# else:\n",
    "#     # Process extracted DataFrames\n",
    "#     for table_name, df in raw_data_dfs.items():\n",
    "#         if df is None:\n",
    "#             print(f\"Error: DataFrame for '{table_name}' is None.\")\n",
    "#         else:\n",
    "#             print(f\"\\nDataFrame for '{table_name}':\")\n",
    "#             print(df.head(2))  # Display first few rows as a check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[CSV Files]**\n",
    "\n",
    "**Customers**\n",
    "* SignupDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "\n",
    "**Subscriptions**\n",
    "* StartDate,EndDate: will be converted to datetime (to be used in PostgreSQL).\n",
    "* Status: will be converted from boolean to integer (to be used in PostgreSQL).\n",
    "\n",
    "**Product Usage**\n",
    "* DateID : no need to converted to datetime (to be used in PostgreSQL) because it is not used in the transformation layer.\n",
    "\n",
    "**Support Interactions**\n",
    "* DateID : no need to converted to datetime (to be used in PostgreSQL) because it is not used in the transformation layer.\n",
    "\n",
    "**Dates**\n",
    "* DateID : no need to converted to datetime (to be used in PostgreSQL) because it is not used in the transformation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_raw(data_frames, date_columns_map):\n",
    "    \"\"\"\n",
    "    Transform multiple raw DataFrames extracted from CSV files.\n",
    "    Perform cleaning procedures: Convert specified date columns to pandas datetime and \n",
    "    boolean columns to integer.\n",
    "\n",
    "    Args:\n",
    "        data_frames (dict): A dictionary where keys are table names and values are DataFrames \n",
    "                            containing raw data extracted from CSV files.\n",
    "        date_columns_map (dict): A dictionary where keys are table names and values are \n",
    "                                 column names to convert to pandas datetime.\n",
    "        boolean_columns_map (dict): (I ommited this argument as there is no boolean column at this time) A dictionary where keys are table names and values are \n",
    "                                    column names to convert from boolean to integer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are table names and values are cleaned DataFrames \n",
    "              with the specified transformations applied.\n",
    "    \"\"\"\n",
    "    cleaned_data_frames = {}\n",
    "\n",
    "    print(\"Transform Function.\")\n",
    "    for table_name, df in data_frames.items():\n",
    "        if table_name not in date_columns_map:\n",
    "            print(f\"Skipping table '{table_name}' due to missing date column mapping.\")\n",
    "            continue\n",
    "        \n",
    "        # If there is a boolean, column, use this below intead of the above\n",
    "        # if table_name not in date_columns_map or table_name not in boolean_columns_map:\n",
    "        #     print(f\"Skipping table '{table_name}' due to missing date or boolean column mapping.\")\n",
    "        #     continue\n",
    "\n",
    "        # Specifying the date and boolean columns in each table to perform transformations\n",
    "        date_columns = date_columns_map[table_name]\n",
    "        # date_columns = date_columns_map.get(table_name)\n",
    "        # There is no boolean column at this time\n",
    "        # boolean_column = boolean_columns_map[table_name]\n",
    "        \n",
    "        # Ensure that the date_columns variable is always treated as a list, even if a single date column name is provided.\n",
    "        if not isinstance(date_columns, list):\n",
    "            date_columns = [date_columns]\n",
    "\n",
    "        try:\n",
    "            for date_column in date_columns:\n",
    "                if date_column not in df.columns:\n",
    "                    raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                \n",
    "                # Format the date column to 'YYYY-MM-DD' format\n",
    "                df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "                print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "                print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "            \n",
    "            cleaned_data_frames[table_name] = df\n",
    "\n",
    "        # # Inner function to convert date to pandas datetime\n",
    "        # def convert_date_to_string(df, date_columns, table_name):\n",
    "        #     \"\"\"\n",
    "        #     Convert the specified date columns in the DataFrame to a string format\n",
    "        #     suitable for PostgreSQL date insertion.\n",
    "\n",
    "        #     Args:\n",
    "        #         df (DataFrame): The DataFrame containing the date columns.\n",
    "        #         date_columns (list): A list of date column names to be converted.\n",
    "        #         table_name (str): The name of the table being processed.\n",
    "\n",
    "        #     Returns:\n",
    "        #         df (DataFrame): The DataFrame with the date columns converted to string format.\n",
    "        #     \"\"\"\n",
    "        #     try:\n",
    "        #         for date_column in date_columns:\n",
    "        #             if date_column not in df.columns:\n",
    "        #                 raise ValueError(f\"Column '{date_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                    \n",
    "        #             # Format the date column to 'YYYY-MM-DD' format\n",
    "        #             df[date_column] = pd.to_datetime(df[date_column]).dt.strftime('%Y-%m-%d')\n",
    "        #             print(f\"Successfully converted column '{date_column}' to 'YYYY-MM-DD' format for table '{table_name}'.\")\n",
    "        #             print(f\"Data type after conversion: {df[date_column].dtype}\")\n",
    "        #     except ValueError as ve:\n",
    "        #         print(ve)\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "        #     return df\n",
    "\n",
    "        # # Call the convert_date_to_string function\n",
    "        # converted_date_df = convert_date_to_string(df, date_columns, table_name)\n",
    "\n",
    "        # if converted_date_df is None:\n",
    "        #     print(f\"Error occurred during date conversion for table '{table_name}'. Cleaning process aborted.\")\n",
    "        #     return None\n",
    "\n",
    "        # There is no boolean column at this time\n",
    "        # # Inner function to convert boolean to integer\n",
    "        # def convert_boolean_to_integer(df, boolean_column):\n",
    "        #     try:\n",
    "        #         if boolean_column not in df.columns:\n",
    "        #             raise ValueError(f\"Column '{boolean_column}' does not exist in the DataFrame for table '{table_name}'.\")\n",
    "                \n",
    "        #         if df[boolean_column].dtype != 'bool':\n",
    "        #             raise TypeError(f\"Column '{boolean_column}' is not of boolean type for table '{table_name}'.\")\n",
    "                \n",
    "        #         df[boolean_column] = df[boolean_column].astype(int)\n",
    "        #         print(f\"Successfully converted column '{boolean_column}' from boolean to integer for table '{table_name}'.\")\n",
    "            \n",
    "        #     except ValueError as ve:\n",
    "        #         print(ve)\n",
    "        #         return None\n",
    "        #     except TypeError as te:\n",
    "        #         print(te)\n",
    "        #         return None\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"An unexpected error occurred during boolean to integer conversion for table '{table_name}': {e}\")\n",
    "        #         return None\n",
    "        \n",
    "        #     return  df\n",
    "\n",
    "        # # Call the convert_boolean_to_integer function\n",
    "        # cleaned_data_final_df = convert_boolean_to_integer(converted_date_df, boolean_column)\n",
    "        # print(cleaned_data_final_df.dtypes)\n",
    "\n",
    "        # if cleaned_data_final_df is None:\n",
    "        #     print(f\"Error occurred during boolean to integer conversion for table '{table_name}'. Cleaning process aborted.\")\n",
    "        #     return None\n",
    "        \n",
    "        # Store the cleaned DataFrame for table '{table_name}' in the dictionary\n",
    "        # cleaned_data_frames[table_name] = converted_date_df\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred when converting the date for table '{table_name}': {e}\")\n",
    "            cleaned_data_frames[table_name] = None\n",
    "\n",
    "    return cleaned_data_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the transform function\n",
    "\n",
    "# csv_folder_path = '/workspace/data/raw'\n",
    "# raw_data_dfs = extract(csv_folder_path)\n",
    "# date_columns_map = {'customers': 'SignupDate', 'subscriptions': ['StartDate', 'EndDate']}\n",
    "# # There is no boolean column at this time\n",
    "# # boolean_columns_map = {'subscriptions': 'Status'}\n",
    "\n",
    "# transformed_data_dfs = transform_raw(raw_data_dfs, date_columns_map)\n",
    "\n",
    "# if transformed_data_dfs is None:\n",
    "#     print(\"Error occurred during transformation. Processing aborted.\")\n",
    "# else:\n",
    "#     for table_name, df in transformed_data_dfs.items():\n",
    "#         print(f\"Transformed DataFrame for table '{table_name}':\")\n",
    "#         print(df.head(2))  # Display first few rows as a check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve individual components from environment variables\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if not all([user, password, host, db_name]):\n",
    "    raise ValueError(\"One or more environment variables for the database connection are not set\")\n",
    "\n",
    "# Construct the connection URI\n",
    "connection_uri = f\"postgresql://{user}:{password}@{host}/{db_name}\"\n",
    "\n",
    "# Ensure the connection URI is retrieved successfully\n",
    "if connection_uri is None:\n",
    "    raise ValueError(\"DATABASE_URL environment variable is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Function that created the SQL Engine based on SQLAlchemy\n",
    "def create_db_engine(connection_uri: str):\n",
    "    \"\"\"\n",
    "    Create and return a SQLAlchemy engine based on the provided connection URI.\n",
    "\n",
    "    Args:\n",
    "        connection_uri (str): The connection URI for the database.\n",
    "\n",
    "    Returns:\n",
    "        Engine: A SQLAlchemy engine connected to the specified database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_engine = create_engine(connection_uri)\n",
    "        print(\"Database engine created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while creating the database engine: {str(e)}\")\n",
    "        return None\n",
    "    # Log or handle the error as needed\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    return db_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name, table_name, date_columns):\n",
    "#     \"\"\"\n",
    "#     Extract, transform, and ingest CSV data into the Bronze layer of a PostgreSQL database.\n",
    "\n",
    "#     Args:\n",
    "#         csv_folder_path (str): Path to the folder containing CSV files.\n",
    "#         connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "#         schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "#         table_name (str): Name of the table to ingest data into.\n",
    "#         date_columns (list or str): Name(s) of the column(s) to convert to pandas datetime.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     # Calling the Extract Function for all CSV files\n",
    "#     print(\"Ingest Function.\")\n",
    "#     raw_data_dfs = extract(csv_folder_path)\n",
    "\n",
    "#     if raw_data_dfs is None:\n",
    "#         # Handle case where extraction fails\n",
    "#         print(\"Extraction failed.\")\n",
    "#     else:\n",
    "#         # Process extracted DataFrames\n",
    "#         for table_name, df in raw_data_dfs.items():\n",
    "#             if df is None:\n",
    "#                 print(f\"Error: DataFrame for '{table_name}' is None.\")\n",
    "#             else:\n",
    "#                 print(f\"\\n (CSV) Extracted DataFrame for '{table_name}':\")\n",
    "#                 print(df.head(2))  # Display first few rows as a check\n",
    "\n",
    "#     # Calling the Transformation Function\n",
    "#     print(\"Transformation Function.\")\n",
    "#     # There is no boolean column at this time\n",
    "#     # boolean_columns_map = {'subscriptions': 'Status'}\n",
    "    \n",
    "#     transformed_data_dfs = transform_raw(raw_data_dfs, {table_name: date_columns})\n",
    "\n",
    "#     if transformed_data_dfs is None:\n",
    "#         print(\"Error occurred during transformation. Processing aborted.\")\n",
    "#     else:\n",
    "#         for table_name, df in transformed_data_dfs.items():\n",
    "#             print(f\"Transformed DataFrame for table '{table_name}':\")\n",
    "#             print(df.head(2))  # Display first few rows as a check\n",
    "\n",
    "#     try:  \n",
    "#         # Create the database engine\n",
    "#         db_engine = create_db_engine(connection_uri)\n",
    "#         if db_engine is None:\n",
    "#             print(\"Failed to create the database engine.\")\n",
    "#             return\n",
    "        \n",
    "#         # Verify connection and schema existence\n",
    "#         with db_engine.connect() as connection:\n",
    "#             # Check if the schema exists\n",
    "#             result = connection.execute(\n",
    "#                 text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "#                 {\"schema\": schema_name}\n",
    "#             )\n",
    "#             schema_exists = result.fetchone() is not None\n",
    "#             if not schema_exists:\n",
    "#                 raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "#             print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "#             # Set the search path to the specified schema\n",
    "#             connection.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "#             print(f\"Search path set to schema '{schema_name}'.\")\n",
    "\n",
    "#             # Iterate over transformed DataFrames and ingest data into the database\n",
    "#             for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "#                 print(f\"Ingesting data into {schema_name}.{table_name}...\")\n",
    "            \n",
    "#             # Generate and execute SQL insert statements for each row in the DataFrame\n",
    "#             for index, row in cleaned_data_df.iterrows():\n",
    "#                 values = \", \".join([f\"'{value}'\" if isinstance(value, str) else str(value) for value in row.values])\n",
    "#                 insert_statement = f\"INSERT INTO {table_name} ({', '.join(cleaned_data_df.columns)}) VALUES ({values});\"\n",
    "#                 connection.execute(text(insert_statement))\n",
    "\n",
    "#             print(f\"CSV data ingested successfully into {schema_name}.{table_name}.\")\n",
    "\n",
    "#             # Commit the transaction\n",
    "#             connection.commit()\n",
    "#             print(\"Transaction committed successfully!\")\n",
    "\n",
    "#             # Query to verify the data was inserted\n",
    "#             for table_name in transformed_data_dfs.keys():\n",
    "#                 verification_query = f\"SELECT * FROM {table_name} LIMIT 2;\"\n",
    "#                 result = connection.execute(text(verification_query))\n",
    "#                 data = result.fetchall()\n",
    "#                 print(f\"Verification Query Result for table '{table_name}': {data}\")\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"Ingest Function: Error - CSV file not found.\")\n",
    "#     except SQLAlchemyError as e:\n",
    "#         print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "#     except ValueError as ve:\n",
    "#         print(f\"ValueError: {str(ve)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name, table_name, date_columns):\n",
    "    \"\"\"\n",
    "    Extract, transform, and ingest CSV data into the Bronze layer of a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "        csv_folder_path (str): Path to the folder containing CSV files.\n",
    "        connection_uri (str): Connection URI for the PostgreSQL database.\n",
    "        schema_name (str): Name of the schema in which tables exist or will be created.\n",
    "        table_name (str): Name of the table to ingest data into.\n",
    "        date_columns (list or str): Name(s) of the column(s) to convert to pandas datetime.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calling the Extract Function for all CSV files\n",
    "        print(\"Ingest Function.\")\n",
    "        raw_data_dfs = extract(csv_folder_path)\n",
    "\n",
    "        if raw_data_dfs is None:\n",
    "            # Handle case where extraction fails\n",
    "            print(\"Extraction failed.\")\n",
    "            return\n",
    "\n",
    "        # Calling the Transformation Function\n",
    "        print(\"Transformation Function.\")\n",
    "        transformed_data_dfs = transform_raw(raw_data_dfs, {table_name: date_columns})\n",
    "\n",
    "        if transformed_data_dfs is None:\n",
    "            print(\"Error occurred during transformation. Processing aborted.\")\n",
    "            return\n",
    "\n",
    "        # Create the database engine\n",
    "        db_engine = create_db_engine(connection_uri)\n",
    "        if db_engine is None:\n",
    "            print(\"Failed to create the database engine.\")\n",
    "            return\n",
    "\n",
    "        with db_engine.connect() as connection:\n",
    "            # Check if the schema exists\n",
    "            result = connection.execute(\n",
    "                text(f\"SELECT schema_name FROM information_schema.schemata WHERE schema_name = :schema\"),\n",
    "                {\"schema\": schema_name}\n",
    "            )\n",
    "            schema_exists = result.fetchone() is not None\n",
    "            if not schema_exists:\n",
    "                raise ValueError(f\"Schema '{schema_name}' does not exist in the database.\")\n",
    "            print(f\"Schema '{schema_name}' verified to exist.\")\n",
    "\n",
    "            # Set the search path to the specified schema\n",
    "            connection.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "            print(f\"Search path set to schema '{schema_name}'.\")\n",
    "\n",
    "            # Iterate over transformed DataFrames and ingest data into the database\n",
    "            for table_name, cleaned_data_df in transformed_data_dfs.items():\n",
    "                print(f\"Ingesting data into {schema_name}.{table_name}...\")\n",
    "\n",
    "                # Generate and execute SQL insert statements for each row in the DataFrame\n",
    "                for index, row in cleaned_data_df.iterrows():\n",
    "                    values = \", \".join([f\"'{value}'\" if isinstance(value, str) else str(value) for value in row.values])\n",
    "                    insert_statement = f\"INSERT INTO {table_name} ({', '.join(cleaned_data_df.columns)}) VALUES ({values});\"\n",
    "                    connection.execute(text(insert_statement))\n",
    "\n",
    "                print(f\"CSV data ingested successfully into {schema_name}.{table_name}.\")\n",
    "\n",
    "                # Commit the transaction\n",
    "                connection.commit()\n",
    "                print(\"Transaction committed successfully!\")\n",
    "\n",
    "                # Query to verify the data was inserted\n",
    "                verification_query = f\"SELECT * FROM {table_name} LIMIT 2;\"\n",
    "                result = connection.execute(text(verification_query))\n",
    "                data = result.fetchall()\n",
    "                print(f\"Verification Query Result for table '{table_name}': {data}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ingest Function: Error - CSV file not found.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Error occurred while connecting to the database or ingesting data: {str(e)}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {str(ve)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting 'customers' into bronze...\n",
      "Ingest Function.\n",
      "Extract Function.\n",
      "CSV file 'customers.csv' loaded successfully.\n",
      "CSV file 'dates.csv' loaded successfully.\n",
      "CSV file 'product_usage.csv' loaded successfully.\n",
      "CSV file 'subscriptions.csv' loaded successfully.\n",
      "CSV file 'support_interactions.csv' loaded successfully.\n",
      "Transformation Function.\n",
      "Transform Function.\n",
      "Successfully converted column 'SignupDate' to 'YYYY-MM-DD' format for table 'customers'.\n",
      "Data type after conversion: object\n",
      "Skipping table 'dates' due to missing date column mapping.\n",
      "Skipping table 'product_usage' due to missing date column mapping.\n",
      "Skipping table 'subscriptions' due to missing date column mapping.\n",
      "Skipping table 'support_interactions' due to missing date column mapping.\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' verified to exist.\n",
      "Search path set to schema 'bronze'.\n",
      "Ingesting data into bronze.customers...\n",
      "CSV data ingested successfully into bronze.customers.\n",
      "Transaction committed successfully!\n",
      "Verification Query Result for table 'customers': [(101, 'John Doe', 30, 'M', datetime.date(2020, 1, 15)), (102, 'Jane Smith', 25, 'F', datetime.date(2019, 6, 20))]\n",
      "Ingesting 'subscriptions' into bronze...\n",
      "Ingest Function.\n",
      "Extract Function.\n",
      "CSV file 'customers.csv' loaded successfully.\n",
      "CSV file 'dates.csv' loaded successfully.\n",
      "CSV file 'product_usage.csv' loaded successfully.\n",
      "CSV file 'subscriptions.csv' loaded successfully.\n",
      "CSV file 'support_interactions.csv' loaded successfully.\n",
      "Transformation Function.\n",
      "Transform Function.\n",
      "Skipping table 'customers' due to missing date column mapping.\n",
      "Skipping table 'dates' due to missing date column mapping.\n",
      "Skipping table 'product_usage' due to missing date column mapping.\n",
      "Successfully converted column 'StartDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Successfully converted column 'EndDate' to 'YYYY-MM-DD' format for table 'subscriptions'.\n",
      "Data type after conversion: object\n",
      "Skipping table 'support_interactions' due to missing date column mapping.\n",
      "Database engine created successfully.\n",
      "Schema 'bronze' verified to exist.\n",
      "Search path set to schema 'bronze'.\n",
      "Ingesting data into bronze.subscriptions...\n",
      "CSV data ingested successfully into bronze.subscriptions.\n",
      "Transaction committed successfully!\n",
      "Verification Query Result for table 'subscriptions': [(201, 101, datetime.date(2020, 1, 15), datetime.date(2021, 1, 14), 'Annual', 'Active'), (202, 102, datetime.date(2019, 6, 20), datetime.date(2020, 6, 19), 'Monthly', 'Churned')]\n",
      "All data ingested successfully into the Bronze Layer!\n"
     ]
    }
   ],
   "source": [
    "# Bronze Ingestion parameters\n",
    "csv_folder_path = '/workspace/data/raw'\n",
    "schema_name_bronze = 'bronze'\n",
    "# schema_name_silver = 'silver'\n",
    "# schema_name_gold = 'gold'\n",
    "\n",
    "# Tables in each schema\n",
    "tables_in_bronze = ['customers', 'subscriptions']\n",
    "# tables_in_silver = ['customer_transactions', 'customer_activity']\n",
    "# tables_in_gold = ['customer_segmentation', 'churn_prediction']\n",
    "\n",
    "# Define date columns for each table\n",
    "date_columns_map = {\n",
    "    'customers': 'SignupDate',\n",
    "    'subscriptions': ['StartDate', 'EndDate']\n",
    "}\n",
    "\n",
    "# Ingest data into Bronze layer\n",
    "for table_name in tables_in_bronze:\n",
    "    print(f\"Ingesting '{table_name}' into {schema_name_bronze}...\")\n",
    "    ingest_csv_to_bronze(csv_folder_path, connection_uri, schema_name_bronze, table_name, date_columns_map[table_name])\n",
    "\n",
    "# Ingest data into Silver layer\n",
    "# for table_name in tables_in_silver:\n",
    "#     print(f\"Ingesting '{table_name}' from {schema_name_bronze} to {schema_name_silver}...\")\n",
    "#     ingest_bronze_to_silver(connection_uri, schema_name_bronze, schema_name_silver, date_columns_map[table_name])\n",
    "\n",
    "# Ingest data into Gold layer\n",
    "# for table_name in tables_in_gold:\n",
    "#     print(f\"Ingesting '{table_name}' from {schema_name_silver} to {schema_name_gold}...\")\n",
    "#     ingest_silver_to_gold(connection_uri, schema_name_silver, schema_name_gold, date_columns_map[table_name])\n",
    "\n",
    "print(\"All data ingested successfully into the Bronze Layer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
